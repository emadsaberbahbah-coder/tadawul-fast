# =============================================================================
# Tadawul Fast Bridge - Main Application
# Version: 3.6.0
# Python: 3.11.x
# Author: Production Team
# Description: Enhanced high-performance bridge for Tadawul stock data processing
# =============================================================================

import asyncio
import json
import time
import traceback
from datetime import datetime, timedelta, timezone
from typing import Dict, List, Optional, Any, Union
from contextlib import asynccontextmanager

# Core Framework
from fastapi import FastAPI, Request, Response, HTTPException, Depends, status, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse, HTMLResponse
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from fastapi.middleware.gzip import GZipMiddleware
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded

# Data Models & Configuration
from pydantic import BaseModel, Field, validator, ConfigDict
from pydantic_settings import BaseSettings

# HTTP Clients
import httpx
import aiohttp
import requests

# Data Processing
import pandas as pd
import numpy as np

# Google Services
import gspread
from google.oauth2.service_account import Credentials
from google.auth.transport.requests import Request as GoogleRequest

# Web Scraping
from bs4 import BeautifulSoup

# Error Handling & Resilience
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

# Monitoring & Logging
import structlog
from prometheus_client import Counter, Histogram, Gauge, generate_latest, CONTENT_TYPE_LATEST, REGISTRY

# Security
from jose import JWTError, jwt
from passlib.context import CryptContext
from cryptography.fernet import Fernet

# Utilities
import orjson
from cachetools import TTLCache
import pytz
from dateutil import parser
import yaml
from dotenv import load_dotenv

# =============================================================================
# Configuration & Settings
# =============================================================================

load_dotenv()

class Settings(BaseSettings):
    """Application configuration settings."""
    
    # Application
    app_name: str = "Tadawul Fast Bridge"
    app_version: str = "3.6.0"
    environment: str = "production"
    debug: bool = False
    
    # Server
    host: str = "0.0.0.0"
    port: int = 8000
    workers: int = 1
    reload: bool = False
    
    # Security
    secret_key: str = Field(default="development-secret-key-change-in-production", env="SECRET_KEY")
    algorithm: str = "HS256"
    access_token_expire_minutes: int = 30
    
    # API Keys
    argaam_api_key: Optional[str] = Field(None, env="ARGAAM_API_KEY")
    google_sheets_credentials: Optional[str] = Field(None, env="GOOGLE_CREDENTIALS_JSON")
    
    # Rate Limiting
    rate_limit_per_minute: int = 60
    rate_limit_per_hour: int = 1000
    
    # Cache
    cache_ttl_seconds: int = 300
    cache_max_size: int = 1000
    redis_url: Optional[str] = Field(None, env="REDIS_URL")
    
    # External APIs
    argaam_base_url: str = "https://api.argaam.com/v1.0"
    tadawul_base_url: str = "https://www.tadawul.com.sa"
    
    # Monitoring
    enable_prometheus: bool = True
    enable_health_checks: bool = True
    log_level: str = "INFO"
    
    # Data Processing
    max_batch_size: int = 50
    data_retention_days: int = 30
    
    # Performance
    request_timeout: int = 30
    max_concurrent_requests: int = 100
    
    model_config = ConfigDict(
        env_file=".env",
        env_file_encoding="utf-8",
        case_sensitive=False,
        extra="ignore"
    )

# =============================================================================
# Logging Configuration
# =============================================================================

def setup_logging(settings: Settings):
    """Configure structured logging."""
    structlog.configure(
        processors=[
            structlog.stdlib.filter_by_level,
            structlog.stdlib.add_logger_name,
            structlog.stdlib.add_log_level,
            structlog.processors.TimeStamper(fmt="iso"),
            structlog.processors.StackInfoRenderer(),
            structlog.processors.format_exc_info,
            structlog.processors.JSONRenderer()
        ],
        wrapper_class=structlog.make_filtering_bound_logger(
            getattr(structlog.stdlib, settings.log_level.upper())
        ),
        context_class=dict,
        logger_factory=structlog.stdlib.LoggerFactory(),
        cache_logger_on_first_use=True,
    )
    return structlog.get_logger()

# =============================================================================
# Models
# =============================================================================

class StockSymbol(BaseModel):
    """Stock symbol model."""
    symbol: str = Field(..., description="Tadawul symbol code")
    name_ar: str = Field(..., description="Arabic name")
    name_en: str = Field(..., description="English name")
    sector: str = Field(..., description="Market sector")
    market_cap: Optional[float] = Field(None, description="Market capitalization")
    last_price: Optional[float] = Field(None, description="Last traded price")
    change: Optional[float] = Field(None, description="Price change")
    change_percent: Optional[float] = Field(None, description="Percentage change")
    volume: Optional[int] = Field(None, description="Traded volume")
    
    model_config = ConfigDict(
        json_encoders={
            datetime: lambda v: v.isoformat(),
            pd.Timestamp: lambda v: v.isoformat()
        }
    )

class MarketData(BaseModel):
    """Market data model."""
    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    market_index: float
    market_change: float
    market_change_percent: float
    turnover: float
    volume: int
    advancers: int
    decliners: int
    unchanged: int
    
    model_config = ConfigDict(
        json_encoders={
            datetime: lambda v: v.isoformat()
        }
    )

class TechnicalIndicator(BaseModel):
    """Technical indicator model."""
    symbol: str
    indicator: str
    value: float
    signal: str = Field(..., pattern="^(BUY|SELL|NEUTRAL)$")
    timestamp: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    
    model_config = ConfigDict(
        json_encoders={
            datetime: lambda v: v.isoformat()
        }
    )

class TokenData(BaseModel):
    """JWT token data model."""
    username: Optional[str] = None
    user_id: Optional[str] = None
    scopes: List[str] = []

# =============================================================================
# Security & Authentication
# =============================================================================

security = HTTPBearer()
pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")

def verify_password(plain_password: str, hashed_password: str) -> bool:
    """Verify password hash."""
    return pwd_context.verify(plain_password, hashed_password)

def get_password_hash(password: str) -> str:
    """Generate password hash."""
    return pwd_context.hash(password)

def create_access_token(
    data: dict,
    expires_delta: Optional[timedelta] = None,
    settings: Settings = None
) -> str:
    """Create JWT access token."""
    to_encode = data.copy()
    if expires_delta:
        expire = datetime.now(timezone.utc) + expires_delta
    else:
        expire = datetime.now(timezone.utc) + timedelta(
            minutes=settings.access_token_expire_minutes
        )
    to_encode.update({"exp": expire})
    return jwt.encode(
        to_encode,
        settings.secret_key,
        algorithm=settings.algorithm
    )

async def get_current_user(
    credentials: HTTPAuthorizationCredentials = Depends(security),
    settings: Settings = None
) -> TokenData:
    """Validate JWT token and return current user."""
    if not credentials:
        return TokenData(username="anonymous", scopes=["public"])
    
    credentials_exception = HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Could not validate credentials",
        headers={"WWW-Authenticate": "Bearer"},
    )
    
    try:
        payload = jwt.decode(
            credentials.credentials,
            settings.secret_key,
            algorithms=[settings.algorithm]
        )
        username: str = payload.get("sub")
        if username is None:
            raise credentials_exception
        return TokenData(username=username)
    except JWTError:
        raise credentials_exception

# =============================================================================
# Rate Limiting
# =============================================================================

limiter = Limiter(key_func=get_remote_address)

# =============================================================================
# Prometheus Metrics
# =============================================================================

REQUEST_COUNT = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status']
)

REQUEST_LATENCY = Histogram(
    'http_request_duration_seconds',
    'HTTP request latency',
    ['method', 'endpoint']
)

STOCK_DATA_FETCHES = Counter(
    'stock_data_fetches_total',
    'Total stock data fetch operations',
    ['source', 'symbol']
)

GOOGLE_SHEET_UPDATES = Counter(
    'google_sheet_updates_total',
    'Total Google Sheet update operations'
)

API_ERRORS = Counter(
    'api_errors_total',
    'Total API errors',
    ['error_type', 'endpoint']
)

# =============================================================================
# Cache Implementation
# =============================================================================

class DataCache:
    """TTL-based data cache."""
    
    def __init__(self, maxsize: int = 1000, ttl: int = 300):
        self.cache = TTLCache(maxsize=maxsize, ttl=ttl)
    
    def get(self, key: str) -> Optional[Any]:
        """Get value from cache."""
        return self.cache.get(key)
    
    def set(self, key: str, value: Any):
        """Set value in cache."""
        self.cache[key] = value
    
    def clear(self):
        """Clear cache."""
        self.cache.clear()

# =============================================================================
# Google Sheets Service
# =============================================================================

class GoogleSheetsService:
    """Google Sheets integration service."""
    
    def __init__(self, credentials_json: Optional[str] = None):
        self.credentials_json = credentials_json
        self.client = None
        self._init_client()
    
    def _init_client(self):
        """Initialize Google Sheets client."""
        if not self.credentials_json:
            return
        
        try:
            creds_dict = json.loads(self.credentials_json)
            creds = Credentials.from_service_account_info(creds_dict)
            self.client = gspread.authorize(creds)
        except Exception as e:
            logger.error(f"Failed to initialize Google Sheets client: {e}")
            self.client = None
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10),
        retry=retry_if_exception_type((gspread.exceptions.APIError, Exception))
    )
    async def update_sheet(
        self,
        spreadsheet_id: str,
        sheet_name: str,
        data: List[List[Any]]
    ) -> Dict[str, Any]:
        """Update Google Sheet with data."""
        if not self.client:
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="Google Sheets service not configured"
            )
        
        try:
            spreadsheet = self.client.open_by_key(spreadsheet_id)
            worksheet = spreadsheet.worksheet(sheet_name)
            
            worksheet.clear()
            worksheet.update(data, value_input_option='USER_ENTERED')
            
            GOOGLE_SHEET_UPDATES.inc()
            logger.info(f"Updated Google Sheet: {spreadsheet_id}/{sheet_name}")
            
            return {
                "status": "success",
                "rows_updated": len(data)
            }
            
        except gspread.exceptions.APIError as e:
            API_ERRORS.labels(error_type="google_sheets", endpoint="update_sheet").inc()
            logger.error(f"Google Sheets API error: {e}")
            raise HTTPException(
                status_code=status.HTTP_502_BAD_GATEWAY,
                detail=f"Google Sheets API error: {str(e)}"
            )

# =============================================================================
# HTTP Client with Retry Logic
# =============================================================================

class ResilientHTTPClient:
    """HTTP client with built-in retry and error handling."""
    
    def __init__(self):
        self.session = None
    
    async def __aenter__(self):
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            headers={
                'User-Agent': 'TadawulFastBridge/3.6.0',
                'Accept': 'application/json'
            }
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type((aiohttp.ClientError, asyncio.TimeoutError))
    )
    async def fetch_json(
        self,
        url: str,
        params: Optional[Dict] = None,
        headers: Optional[Dict] = None
    ) -> Dict:
        """Fetch JSON data with retry logic."""
        start_time = time.time()
        
        try:
            request_headers = {}
            if headers:
                request_headers.update(headers)
            
            async with self.session.get(
                url,
                params=params,
                headers=request_headers
            ) as response:
                
                REQUEST_COUNT.labels(
                    method='GET',
                    endpoint=url,
                    status=response.status
                ).inc()
                
                if response.status == 200:
                    data = await response.json()
                    latency = time.time() - start_time
                    REQUEST_LATENCY.labels(
                        method='GET',
                        endpoint=url
                    ).observe(latency)
                    return data
                else:
                    raise HTTPException(
                        status_code=response.status,
                        detail=f"HTTP error {response.status}"
                    )
        except asyncio.TimeoutError:
            API_ERRORS.labels(error_type="timeout", endpoint=url).inc()
            raise HTTPException(
                status_code=status.HTTP_504_GATEWAY_TIMEOUT,
                detail="Request timeout"
            )

# =============================================================================
# Stock Data Service
# =============================================================================

class StockDataService:
    """Stock data aggregation and processing service."""
    
    def __init__(self, cache: DataCache):
        self.http_client = ResilientHTTPClient()
        self.sheets_service = GoogleSheetsService()
        self.cache = cache
    
    async def get_symbol_data(
        self,
        symbol: str,
        use_cache: bool = True
    ) -> Dict:
        """Get detailed data for a specific symbol."""
        symbol = symbol.strip().upper()
        cache_key = f"symbol:{symbol}"
        
        if use_cache:
            cached = self.cache.get(cache_key)
            if cached:
                logger.debug(f"Cache hit for symbol: {symbol}")
                return cached
        
        try:
            # Try Argaam API first
            if settings.argaam_api_key:
                async with self.http_client as client:
                    data = await client.fetch_json(
                        f"{settings.argaam_base_url}/symbols/{symbol}",
                        params={"apikey": settings.argaam_api_key}
                    )
                
                processed_data = self._process_argaam_data(data, symbol)
            else:
                # Fallback to mock data
                processed_data = self._generate_mock_data(symbol)
            
            # Cache the result
            self.cache.set(cache_key, processed_data)
            
            STOCK_DATA_FETCHES.labels(source="argaam", symbol=symbol).inc()
            
            return processed_data
            
        except Exception as e:
            logger.error(f"Failed to fetch data for {symbol}: {e}")
            # Return mock data as fallback
            mock_data = self._generate_mock_data(symbol)
            self.cache.set(cache_key, mock_data)
            return mock_data
    
    def _process_argaam_data(self, raw_data: Dict, symbol: str) -> Dict:
        """Process Argaam API data."""
        return {
            "symbol": symbol,
            "name_ar": raw_data.get("arabicName", f"شركة {symbol}"),
            "name_en": raw_data.get("englishName", f"Company {symbol}"),
            "sector": raw_data.get("sector", "Unknown"),
            "market_cap": raw_data.get("marketCap"),
            "last_price": raw_data.get("lastPrice"),
            "change": raw_data.get("change"),
            "change_percent": raw_data.get("changePercent"),
            "volume": raw_data.get("volume"),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
    
    def _generate_mock_data(self, symbol: str) -> Dict:
        """Generate mock data for testing."""
        import random
        return {
            "symbol": symbol,
            "name_ar": f"شركة {symbol}",
            "name_en": f"Company {symbol}",
            "sector": random.choice(["البنوك", "البتروكيماويات", "الاتصالات", "التأمين"]),
            "market_cap": random.uniform(10000000, 1000000000),
            "last_price": random.uniform(10, 500),
            "change": random.uniform(-10, 10),
            "change_percent": random.uniform(-5, 5),
            "volume": random.randint(100000, 10000000),
            "timestamp": datetime.now(timezone.utc).isoformat()
        }
    
    async def get_market_summary(self) -> MarketData:
        """Get market summary data."""
        cache_key = "market_summary"
        cached = self.cache.get(cache_key)
        
        if cached:
            return MarketData(**cached)
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    f"{settings.tadawul_base_url}/market-summary",
                    timeout=10
                ) as response:
                    if response.status == 200:
                        html = await response.text()
                        data = self._parse_market_summary(html)
                        self.cache.set(cache_key, data.dict())
                        return data
        except Exception as e:
            logger.warning(f"Failed to fetch market summary: {e}")
        
        # Return mock data
        data = MarketData(
            market_index=11234.56,
            market_change=45.67,
            market_change_percent=0.41,
            turnover=3456789012.34,
            volume=123456789,
            advancers=145,
            decliners=67,
            unchanged=23
        )
        
        self.cache.set(cache_key, data.dict())
        return data
    
    def _parse_market_summary(self, html: str) -> MarketData:
        """Parse market summary from HTML."""
        # Simplified parsing - implement actual parsing as needed
        return MarketData(
            market_index=11234.56,
            market_change=45.67,
            market_change_percent=0.41,
            turnover=3456789012.34,
            volume=123456789,
            advancers=145,
            decliners=67,
            unchanged=23
        )

# =============================================================================
# Application Lifespan Management
# =============================================================================

@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    Manage application lifespan events.
    """
    # Startup
    logger.info(f"Starting {settings.app_name} v{settings.app_version}")
    logger.info(f"Environment: {settings.environment}")
    
    # Initialize services
    cache = DataCache(
        maxsize=settings.cache_max_size,
        ttl=settings.cache_ttl_seconds
    )
    
    app.state.cache = cache
    app.state.stock_service = StockDataService(cache)
    app.state.sheets_service = GoogleSheetsService(
        settings.google_sheets_credentials
    )
    
    # Load initial data
    try:
        await app.state.stock_service.get_market_summary()
        logger.info("Initial market data loaded")
    except Exception as e:
        logger.warning(f"Failed to load initial market data: {e}")
    
    yield
    
    # Shutdown
    logger.info("Shutting down application")
    app.state.cache.clear()

# =============================================================================
# FastAPI Application
# =============================================================================

# Initialize settings
settings = Settings()

# Initialize logging
logger = setup_logging(settings)

app = FastAPI(
    title=settings.app_name,
    version=settings.app_version,
    description="High-performance bridge for Tadawul stock data processing and Google Sheets integration",
    lifespan=lifespan,
    docs_url="/docs",
    redoc_url="/redoc"
)

# Rate limiting exception handler
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Compression middleware
app.add_middleware(GZipMiddleware, minimum_size=1000)

# =============================================================================
# Middleware
# =============================================================================

@app.middleware("http")
async def add_process_time_header(request: Request, call_next):
    """Add processing time header to responses."""
    start_time = time.time()
    response = await call_next(request)
    process_time = time.time() - start_time
    response.headers["X-Process-Time"] = str(process_time)
    return response

@app.middleware("http")
async def log_requests(request: Request, call_next):
    """Log all incoming requests."""
    logger.info(
        "Request started",
        method=request.method,
        url=str(request.url),
        client_host=request.client.host if request.client else None
    )
    
    response = await call_next(request)
    
    logger.info(
        "Request completed",
        method=request.method,
        url=str(request.url),
        status_code=response.status_code
    )
    
    return response

# =============================================================================
# Health & Monitoring Endpoints
# =============================================================================

@app.get("/", tags=["Root"])
async def root():
    """Root endpoint."""
    return {
        "service": settings.app_name,
        "version": settings.app_version,
        "status": "operational",
        "environment": settings.environment,
        "timestamp": datetime.now(timezone.utc).isoformat()
    }

@app.get("/health", tags=["Monitoring"])
async def health_check():
    """Health check endpoint."""
    return {
        "status": "healthy",
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "version": settings.app_version,
        "environment": settings.environment,
        "uptime": time.time() - app.state.start_time if hasattr(app.state, 'start_time') else 0
    }

@app.get("/metrics", tags=["Monitoring"])
async def metrics():
    """Prometheus metrics endpoint."""
    if settings.enable_prometheus:
        return Response(
            content=generate_latest(REGISTRY),
            media_type=CONTENT_TYPE_LATEST
        )
    else:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Metrics endpoint disabled"
        )

# =============================================================================
# Stock Data Endpoints
# =============================================================================

@app.get("/api/v1/symbols/{symbol}", response_model=StockSymbol, tags=["Stock Data"])
@limiter.limit("60/minute")
async def get_symbol(
    request: Request,
    symbol: str,
    use_cache: bool = Query(True, description="Use cached data"),
    current_user: TokenData = Depends(get_current_user)
):
    """
    Get detailed data for a specific Tadawul symbol.
    
    - **symbol**: Tadawul symbol code (e.g., 1211 for SARCO)
    - **use_cache**: Whether to use cached data (default: True)
    """
    service = request.app.state.stock_service
    data = await service.get_symbol_data(symbol, use_cache)
    return StockSymbol(**data)

@app.get("/api/v1/market/summary", response_model=MarketData, tags=["Market Data"])
@limiter.limit("30/minute")
async def get_market_summary(request: Request):
    """Get market summary and statistics."""
    service = request.app.state.stock_service
    data = await service.get_market_summary()
    return data

@app.get("/api/v1/symbols", response_model=List[StockSymbol], tags=["Stock Data"])
@limiter.limit("30/minute")
async def get_multiple_symbols(
    request: Request,
    symbols: str = Query(..., description="Comma-separated list of symbols"),
    use_cache: bool = Query(True, description="Use cached data"),
    current_user: TokenData = Depends(get_current_user)
):
    """
    Get data for multiple symbols.
    
    - **symbols**: Comma-separated list of symbols (e.g., 1211,2222,1180)
    - **use_cache**: Whether to use cached data (default: True)
    """
    symbol_list = [s.strip().upper() for s in symbols.split(',')]
    
    if len(symbol_list) > settings.max_batch_size:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Maximum {settings.max_batch_size} symbols allowed per request"
        )
    
    service = request.app.state.stock_service
    
    results = []
    for symbol in symbol_list:
        try:
            data = await service.get_symbol_data(symbol, use_cache)
            results.append(StockSymbol(**data))
        except Exception as e:
            logger.warning(f"Failed to fetch {symbol}: {e}")
    
    return results

# =============================================================================
# Google Sheets Integration Endpoints
# =============================================================================

@app.post("/api/v1/export/to-sheets", tags=["Export"])
@limiter.limit("10/minute")
async def export_to_sheets(
    request: Request,
    spreadsheet_id: str,
    sheet_name: str = "StockData",
    symbols: Optional[str] = None,
    current_user: TokenData = Depends(get_current_user)
):
    """
    Export stock data to Google Sheets.
    
    - **spreadsheet_id**: Google Sheets spreadsheet ID
    - **sheet_name**: Target sheet name
    - **symbols**: Optional comma-separated symbols (default: market leaders)
    """
    service = request.app.state.stock_service
    sheets_service = request.app.state.sheets_service
    
    # Determine which symbols to export
    if symbols:
        symbol_list = [s.strip().upper() for s in symbols.split(',')]
    else:
        symbol_list = ["1211", "2222", "1180", "4200", "2380"]
    
    # Fetch data for all symbols
    all_data = []
    for symbol in symbol_list:
        try:
            data = await service.get_symbol_data(symbol)
            
            # Convert to row format
            row = [
                data.get('symbol', ''),
                data.get('name_en', ''),
                data.get('last_price', 0),
                data.get('change', 0),
                data.get('change_percent', 0),
                data.get('volume', 0),
                datetime.now(timezone.utc).isoformat()
            ]
            all_data.append(row)
        except Exception as e:
            logger.error(f"Failed to fetch {symbol}: {e}")
            continue
    
    if not all_data:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="No data available for export"
        )
    
    # Add headers
    headers = [
        ["Symbol", "Name", "Last Price", "Change", "Change %", "Volume", "Timestamp"]
    ]
    data_to_export = headers + all_data
    
    # Update Google Sheet
    result = await sheets_service.update_sheet(spreadsheet_id, sheet_name, data_to_export)
    
    return {
        "status": "success",
        "spreadsheet_id": spreadsheet_id,
        "sheet_name": sheet_name,
        "symbols_exported": len(all_data),
        "details": result
    }

# =============================================================================
# Data Export Endpoints
# =============================================================================

@app.get("/api/v1/export/csv/{symbol}", tags=["Export"])
@limiter.limit("20/minute")
async def export_csv(
    symbol: str,
    period: str = "1d",
    current_user: TokenData = Depends(get_current_user)
):
    """
    Export symbol data as CSV.
    
    - **symbol**: Tadawul symbol code
    - **period**: Time period (1d, 1w, 1m, 3m, 1y)
    """
    # Fetch data
    service = app.state.stock_service
    data = await service.get_symbol_data(symbol.upper())
    
    # Convert to DataFrame
    df = pd.DataFrame([data])
    
    # Generate CSV
    csv_data = df.to_csv(index=False)
    
    return StreamingResponse(
        iter([csv_data]),
        media_type="text/csv",
        headers={
            "Content-Disposition": f"attachment; filename={symbol}_data_{datetime.now(timezone.utc).date()}.csv"
        }
    )

# =============================================================================
# Error Handlers
# =============================================================================

@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    """Handle HTTP exceptions with structured logging."""
    logger.error(
        "HTTP exception",
        status_code=exc.status_code,
        detail=exc.detail,
        path=request.url.path
    )
    
    return JSONResponse(
        status_code=exc.status_code,
        content={
            "error": True,
            "code": exc.status_code,
            "message": exc.detail,
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "path": request.url.path
        }
    )

@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    """Handle unexpected exceptions."""
    logger.error(
        "Unhandled exception",
        error_type=type(exc).__name__,
        error_message=str(exc),
        path=request.url.path,
        traceback=traceback.format_exc()
    )
    
    API_ERRORS.labels(
        error_type="unhandled",
        endpoint=request.url.path
    ).inc()
    
    return JSONResponse(
        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
        content={
            "error": True,
            "code": 500,
            "message": "Internal server error",
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "request_id": request.headers.get("X-Request-ID", "unknown")
        }
    )

# =============================================================================
# Configuration Validation
# =============================================================================

def validate_configuration():
    """Validate application configuration."""
    try:
        # Check required settings
        if not settings.secret_key or settings.secret_key == "development-secret-key-change-in-production":
            logger.warning("SECRET_KEY is not set or is default value")
        
        # Check external service configurations
        if not settings.argaam_api_key:
            logger.warning("ARGAAM_API_KEY not set - using mock data")
        
        if not settings.google_sheets_credentials:
            logger.warning("GOOGLE_CREDENTIALS_JSON not set - Google Sheets features disabled")
        
        logger.info("Configuration validation completed")
        
    except Exception as e:
        logger.error(f"Configuration validation failed: {e}")
        raise

# =============================================================================
# Application Initialization
# =============================================================================

# Store app start time
app.state.start_time = time.time()

# Validate configuration on startup
validate_configuration()

# =============================================================================
# Main Entry Point
# =============================================================================

if __name__ == "__main__":
    import uvicorn
    
    logger.info(f"Starting {settings.app_name} on {settings.host}:{settings.port}")
    
    uvicorn.run(
        "main:app",
        host=settings.host,
        port=settings.port,
        reload=settings.environment == "development",
        workers=settings.workers,
        log_config=None,
        access_log=True
    )
