# =============================================================================
# Tadawul Fast Bridge - Main Application
# Version: 3.5.2
# Python: 3.11.x
# Author: Production Team
# Description: High-performance bridge for Tadawul stock data processing
# =============================================================================

import asyncio
import json
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from contextlib import asynccontextmanager

# Core Framework
from fastapi import FastAPI, Request, Response, HTTPException, Depends, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, StreamingResponse
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded

# Data Models & Configuration
from pydantic import BaseModel, Field, validator
from pydantic_settings import BaseSettings
import pydantic

# HTTP Clients
import httpx
import aiohttp
import requests

# Data Processing
import pandas as pd
import numpy as np

# Google Services
import gspread
from google.oauth2.service_account import Credentials
from google.auth.transport.requests import Request as GoogleRequest

# Web Scraping
from bs4 import BeautifulSoup
import lxml

# Error Handling & Resilience
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

# Monitoring & Logging
import structlog
from prometheus_client import Counter, Histogram, generate_latest, CONTENT_TYPE_LATEST

# Security
from jose import JWTError, jwt
from passlib.context import CryptContext

# Utilities
import orjson
from cachetools import TTLCache
import pytz
from dateutil import parser

# =============================================================================
# Configuration & Settings
# =============================================================================

class Settings(BaseSettings):
    """Application configuration settings."""
    
    # Application
    app_name: str = "Tadawul Fast Bridge"
    app_version: str = "3.5.2"
    environment: str = "production"
    
    # Security
    secret_key: str = Field(..., env="SECRET_KEY")
    algorithm: str = "HS256"
    access_token_expire_minutes: int = 30
    
    # API Keys
    argaam_api_key: Optional[str] = Field(None, env="ARGAAM_API_KEY")
    google_sheets_credentials: Optional[str] = Field(None, env="GOOGLE_CREDENTIALS_JSON")
    
    # Rate Limiting
    rate_limit_per_minute: int = 60
    rate_limit_per_hour: int = 1000
    
    # Cache
    cache_ttl_seconds: int = 300
    cache_max_size: int = 1000
    
    # External APIs
    argaam_base_url: str = "https://api.argaam.com/v1.0"
    tadawul_base_url: str = "https://www.tadawul.com.sa"
    
    # Monitoring
    enable_prometheus: bool = True
    log_level: str = "INFO"
    
    class Config:
        env_file = ".env"
        case_sensitive = False

# =============================================================================
# Logging Configuration
# =============================================================================

def setup_logging():
    """Configure structured logging."""
    structlog.configure(
        processors=[
            structlog.stdlib.filter_by_level,
            structlog.stdlib.add_logger_name,
            structlog.stdlib.add_log_level,
            structlog.stdlib.PositionalArgumentsFormatter(),
            structlog.processors.TimeStamper(fmt="iso"),
            structlog.processors.StackInfoRenderer(),
            structlog.processors.format_exc_info,
            structlog.processors.JSONRenderer()
        ],
        context_class=dict,
        logger_factory=structlog.stdlib.LoggerFactory(),
        wrapper_class=structlog.stdlib.BoundLogger,
        cache_logger_on_first_use=True,
    )
    return structlog.get_logger()

logger = setup_logging()

# =============================================================================
# Models
# =============================================================================

class StockSymbol(BaseModel):
    """Stock symbol model."""
    symbol: str = Field(..., description="Tadawul symbol code")
    name_ar: str = Field(..., description="Arabic name")
    name_en: str = Field(..., description="English name")
    sector: str = Field(..., description="Market sector")
    market_cap: Optional[float] = Field(None, description="Market capitalization")
    last_price: Optional[float] = Field(None, description="Last traded price")
    change: Optional[float] = Field(None, description="Price change")
    change_percent: Optional[float] = Field(None, description="Percentage change")
    volume: Optional[int] = Field(None, description="Traded volume")
    
    class Config:
        json_encoders = {
            datetime: lambda v: v.isoformat(),
            pd.Timestamp: lambda v: v.isoformat()
        }

class MarketData(BaseModel):
    """Market data model."""
    timestamp: datetime = Field(default_factory=datetime.utcnow)
    market_index: float
    market_change: float
    market_change_percent: float
    turnover: float
    volume: int
    advancers: int
    decliners: int
    unchanged: int
    
    class Config:
        json_encoders = {
            datetime: lambda v: v.isoformat()
        }

class TechnicalIndicator(BaseModel):
    """Technical indicator model."""
    symbol: str
    indicator: str
    value: float
    signal: str = Field(..., regex="^(BUY|SELL|NEUTRAL)$")
    timestamp: datetime = Field(default_factory=datetime.utcnow)
    
    class Config:
        json_encoders = {
            datetime: lambda v: v.isoformat()
        }

class TokenData(BaseModel):
    """JWT token data model."""
    username: Optional[str] = None
    user_id: Optional[str] = None
    scopes: List[str] = []

# =============================================================================
# Security & Authentication
# =============================================================================

security = HTTPBearer()
pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")

def verify_password(plain_password, hashed_password):
    """Verify password hash."""
    return pwd_context.verify(plain_password, hashed_password)

def get_password_hash(password):
    """Generate password hash."""
    return pwd_context.hash(password)

def create_access_token(data: dict, expires_delta: Optional[timedelta] = None):
    """Create JWT access token."""
    to_encode = data.copy()
    if expires_delta:
        expire = datetime.utcnow() + expires_delta
    else:
        expire = datetime.utcnow() + timedelta(minutes=15)
    to_encode.update({"exp": expire})
    encoded_jwt = jwt.encode(to_encode, settings.secret_key, algorithm=settings.algorithm)
    return encoded_jwt

async def get_current_user(credentials: HTTPAuthorizationCredentials = Depends(security)):
    """Validate JWT token and return current user."""
    credentials_exception = HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Could not validate credentials",
        headers={"WWW-Authenticate": "Bearer"},
    )
    try:
        payload = jwt.decode(
            credentials.credentials, 
            settings.secret_key, 
            algorithms=[settings.algorithm]
        )
        username: str = payload.get("sub")
        if username is None:
            raise credentials_exception
        token_data = TokenData(username=username)
    except JWTError:
        raise credentials_exception
    return token_data

# =============================================================================
# Rate Limiting
# =============================================================================

limiter = Limiter(key_func=get_remote_address)

# =============================================================================
# Prometheus Metrics
# =============================================================================

# Request metrics
REQUEST_COUNT = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status']
)

REQUEST_LATENCY = Histogram(
    'http_request_duration_seconds',
    'HTTP request latency',
    ['method', 'endpoint']
)

# Business metrics
STOCK_DATA_FETCHES = Counter(
    'stock_data_fetches_total',
    'Total stock data fetch operations',
    ['source', 'symbol']
)

GOOGLE_SHEET_UPDATES = Counter(
    'google_sheet_updates_total',
    'Total Google Sheet update operations'
)

# Error metrics
API_ERRORS = Counter(
    'api_errors_total',
    'Total API errors',
    ['error_type', 'endpoint']
)

# =============================================================================
# Cache Implementation
# =============================================================================

class DataCache:
    """TTL-based data cache."""
    
    def __init__(self, maxsize: int = 1000, ttl: int = 300):
        self.cache = TTLCache(maxsize=maxsize, ttl=ttl)
    
    def get(self, key: str) -> Optional[Any]:
        """Get value from cache."""
        return self.cache.get(key)
    
    def set(self, key: str, value: Any):
        """Set value in cache."""
        self.cache[key] = value
    
    def clear(self):
        """Clear cache."""
        self.cache.clear()

cache = DataCache(
    maxsize=settings.cache_max_size,
    ttl=settings.cache_ttl_seconds
)

# =============================================================================
# Google Sheets Service
# =============================================================================

class GoogleSheetsService:
    """Google Sheets integration service."""
    
    def __init__(self, credentials_json: Optional[str] = None):
        self.credentials_json = credentials_json or settings.google_sheets_credentials
        self.client = None
        self._init_client()
    
    def _init_client(self):
        """Initialize Google Sheets client."""
        if not self.credentials_json:
            logger.warning("Google Sheets credentials not configured")
            return
        
        try:
            creds_dict = json.loads(self.credentials_json)
            creds = Credentials.from_service_account_info(creds_dict)
            self.client = gspread.authorize(creds)
            logger.info("Google Sheets client initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize Google Sheets client: {e}")
            self.client = None
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10),
        retry=retry_if_exception_type((gspread.exceptions.APIError, Exception))
    )
    async def update_sheet(self, spreadsheet_id: str, sheet_name: str, data: List[List[Any]]):
        """Update Google Sheet with data."""
        if not self.client:
            raise HTTPException(
                status_code=status.HTTP_503_SERVICE_UNAVAILABLE,
                detail="Google Sheets service not configured"
            )
        
        try:
            spreadsheet = self.client.open_by_key(spreadsheet_id)
            worksheet = spreadsheet.worksheet(sheet_name)
            
            # Clear existing data and update
            worksheet.clear()
            worksheet.update(data, value_input_option='USER_ENTERED')
            
            GOOGLE_SHEET_UPDATES.inc()
            logger.info(f"Updated Google Sheet: {spreadsheet_id}/{sheet_name}")
            
            return {"status": "success", "rows_updated": len(data)}
            
        except gspread.exceptions.APIError as e:
            API_ERRORS.labels(error_type="google_sheets", endpoint="update_sheet").inc()
            logger.error(f"Google Sheets API error: {e}")
            raise HTTPException(
                status_code=status.HTTP_502_BAD_GATEWAY,
                detail=f"Google Sheets API error: {str(e)}"
            )

# =============================================================================
# HTTP Client with Retry Logic
# =============================================================================

class ResilientHTTPClient:
    """HTTP client with built-in retry and error handling."""
    
    def __init__(self):
        self.session = None
    
    async def __aenter__(self):
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            headers={
                'User-Agent': 'TadawulFastBridge/3.5.2',
                'Accept': 'application/json'
            }
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=2, max=10),
        retry=retry_if_exception_type((aiohttp.ClientError, asyncio.TimeoutError))
    )
    async def fetch_json(self, url: str, params: Optional[Dict] = None) -> Dict:
        """Fetch JSON data with retry logic."""
        start_time = time.time()
        
        try:
            async with self.session.get(url, params=params) as response:
                REQUEST_COUNT.labels(
                    method='GET',
                    endpoint=url,
                    status=response.status
                ).inc()
                
                if response.status == 200:
                    data = await response.json()
                    latency = time.time() - start_time
                    REQUEST_LATENCY.labels(
                        method='GET',
                        endpoint=url
                    ).observe(latency)
                    return data
                else:
                    raise HTTPException(
                        status_code=response.status,
                        detail=f"HTTP error {response.status}"
                    )
        except asyncio.TimeoutError:
            API_ERRORS.labels(error_type="timeout", endpoint=url).inc()
            raise HTTPException(
                status_code=status.HTTP_504_GATEWAY_TIMEOUT,
                detail="Request timeout"
            )

# =============================================================================
# Stock Data Service
# =============================================================================

class StockDataService:
    """Stock data aggregation and processing service."""
    
    def __init__(self):
        self.http_client = ResilientHTTPClient()
        self.sheets_service = GoogleSheetsService()
    
    async def get_symbol_data(self, symbol: str, use_cache: bool = True) -> Dict:
        """Get detailed data for a specific symbol."""
        cache_key = f"symbol:{symbol}"
        
        if use_cache:
            cached = cache.get(cache_key)
            if cached:
                logger.debug(f"Cache hit for symbol: {symbol}")
                return cached
        
        try:
            # Try Argaam API first
            async with self.http_client as client:
                data = await client.fetch_json(
                    f"{settings.argaam_base_url}/symbols/{symbol}",
                    params={"apikey": settings.argaam_api_key}
                )
            
            # Process and enrich data
            processed_data = self._process_symbol_data(data)
            
            # Cache the result
            cache.set(cache_key, processed_data)
            
            STOCK_DATA_FETCHES.labels(source="argaam", symbol=symbol).inc()
            
            return processed_data
            
        except HTTPException:
            # Fallback to web scraping
            logger.warning(f"Falling back to web scraping for {symbol}")
            try:
                data = await self._scrape_symbol_data(symbol)
                cache.set(cache_key, data)
                STOCK_DATA_FETCHES.labels(source="scraping", symbol=symbol).inc()
                return data
            except Exception as e:
                API_ERRORS.labels(error_type="data_fetch", endpoint=f"symbol/{symbol}").inc()
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail=f"Could not fetch data for symbol {symbol}: {str(e)}"
                )
    
    async def get_market_summary(self) -> MarketData:
        """Get market summary data."""
        cache_key = "market_summary"
        cached = cache.get(cache_key)
        
        if cached:
            return cached
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(
                    f"{settings.tadawul_base_url}/market-summary",
                    timeout=10
                ) as response:
                    if response.status == 200:
                        html = await response.text()
                        data = self._parse_market_summary(html)
                        cache.set(cache_key, data)
                        return data
                    else:
                        raise Exception(f"HTTP {response.status}")
        except Exception as e:
            logger.error(f"Failed to fetch market summary: {e}")
            # Return mock data for demonstration
            return MarketData(
                market_index=11234.56,
                market_change=45.67,
                market_change_percent=0.41,
                turnover=3456789012.34,
                volume=123456789,
                advancers=145,
                decliners=67,
                unchanged=23
            )
    
    def _process_symbol_data(self, raw_data: Dict) -> Dict:
        """Process and enrich raw symbol data."""
        df = pd.DataFrame([raw_data])
        
        # Calculate technical indicators
        if 'prices' in raw_data:
            prices = pd.Series(raw_data['prices'])
            df['sma_20'] = prices.rolling(window=20).mean().iloc[-1]
            df['sma_50'] = prices.rolling(window=50).mean().iloc[-1]
            df['rsi'] = self._calculate_rsi(prices)
        
        return df.to_dict(orient='records')[0]
    
    def _calculate_rsi(self, prices: pd.Series, period: int = 14) -> float:
        """Calculate Relative Strength Index."""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi.iloc[-1] if not rsi.empty else 50.0
    
    async def _scrape_symbol_data(self, symbol: str) -> Dict:
        """Fallback method: scrape symbol data from website."""
        url = f"{settings.tadawul_base_url}/symbol/{symbol}"
        
        async with aiohttp.ClientSession() as session:
            async with session.get(url, timeout=10) as response:
                html = await response.text()
        
        soup = BeautifulSoup(html, 'lxml')
        
        # Extract data from HTML (simplified example)
        data = {
            'symbol': symbol,
            'last_price': self._extract_price(soup),
            'volume': self._extract_volume(soup),
            'change': self._extract_change(soup),
            'timestamp': datetime.utcnow().isoformat()
        }
        
        return data
    
    def _parse_market_summary(self, html: str) -> MarketData:
        """Parse market summary from HTML."""
        soup = BeautifulSoup(html, 'lxml')
        
        # Simplified parsing logic
        return MarketData(
            market_index=11234.56,  # Actual parsing would go here
            market_change=45.67,
            market_change_percent=0.41,
            turnover=3456789012.34,
            volume=123456789,
            advancers=145,
            decliners=67,
            unchanged=23
        )
    
    @staticmethod
    def _extract_price(soup: BeautifulSoup) -> float:
        """Extract price from HTML."""
        # Implementation would parse specific elements
        return 123.45
    
    @staticmethod
    def _extract_volume(soup: BeautifulSoup) -> int:
        """Extract volume from HTML."""
        # Implementation would parse specific elements
        return 1000000
    
    @staticmethod
    def _extract_change(soup: BeautifulSoup) -> float:
        """Extract price change from HTML."""
        # Implementation would parse specific elements
        return 1.23

# =============================================================================
# Application Lifespan Management
# =============================================================================

@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    Manage application lifespan events.
    - Startup: Initialize services, load configurations
    - Shutdown: Cleanup resources
    """
    # Startup
    logger.info(f"Starting {settings.app_name} v{settings.app_version}")
    logger.info(f"Environment: {settings.environment}")
    
    # Initialize services
    app.state.stock_service = StockDataService()
    app.state.sheets_service = GoogleSheetsService()
    
    # Load initial data
    try:
        # Pre-cache market summary
        await app.state.stock_service.get_market_summary()
        logger.info("Initial market data loaded")
    except Exception as e:
        logger.warning(f"Failed to load initial market data: {e}")
    
    yield
    
    # Shutdown
    logger.info("Shutting down application")
    cache.clear()

# =============================================================================
# FastAPI Application
# =============================================================================

app = FastAPI(
    title=settings.app_name,
    version=settings.app_version,
    description="High-performance bridge for Tadawul stock data processing and Google Sheets integration",
    lifespan=lifespan
)

# Rate limiting exception handler
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Configure appropriately for production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# =============================================================================
# Middleware
# =============================================================================

@app.middleware("http")
async def add_process_time_header(request: Request, call_next):
    """Add processing time header to responses."""
    start_time = time.time()
    response = await call_next(request)
    process_time = time.time() - start_time
    response.headers["X-Process-Time"] = str(process_time)
    return response

@app.middleware("http")
async def log_requests(request: Request, call_next):
    """Log all incoming requests."""
    logger.info(
        "Request started",
        method=request.method,
        url=str(request.url),
        client_host=request.client.host if request.client else None
    )
    
    response = await call_next(request)
    
    logger.info(
        "Request completed",
        method=request.method,
        url=str(request.url),
        status_code=response.status_code
    )
    
    return response

# =============================================================================
# Health & Monitoring Endpoints
# =============================================================================

@app.get("/health", tags=["Monitoring"])
async def health_check():
    """Health check endpoint."""
    return {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "version": settings.app_version,
        "environment": settings.environment
    }

@app.get("/metrics", tags=["Monitoring"])
async def metrics():
    """Prometheus metrics endpoint."""
    if settings.enable_prometheus:
        return Response(
            content=generate_latest(),
            media_type=CONTENT_TYPE_LATEST
        )
    else:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Metrics endpoint disabled"
        )

@app.get("/status", tags=["Monitoring"])
@limiter.limit("30/minute")
async def system_status(request: Request):
    """System status and statistics."""
    return {
        "cache_size": len(cache.cache),
        "cache_hits": cache.cache.hits,
        "cache_misses": cache.cache.misses,
        "uptime": time.time() - app.state.start_time if hasattr(app.state, 'start_time') else 0,
        "memory_usage": "N/A"  # Could integrate psutil here
    }

# =============================================================================
# Stock Data Endpoints
# =============================================================================

@app.get("/api/v1/symbols/{symbol}", response_model=StockSymbol, tags=["Stock Data"])
@limiter.limit("60/minute")
async def get_symbol(
    request: Request,
    symbol: str,
    use_cache: bool = True,
    current_user: TokenData = Depends(get_current_user)
):
    """
    Get detailed data for a specific Tadawul symbol.
    
    - **symbol**: Tadawul symbol code (e.g., 1211 for SARCO)
    - **use_cache**: Whether to use cached data (default: True)
    """
    service = request.app.state.stock_service
    data = await service.get_symbol_data(symbol.upper(), use_cache)
    return StockSymbol(**data)

@app.get("/api/v1/market/summary", response_model=MarketData, tags=["Market Data"])
@limiter.limit("30/minute")
async def get_market_summary(request: Request):
    """Get market summary and statistics."""
    service = request.app.state.stock_service
    data = await service.get_market_summary()
    return data

@app.get("/api/v1/symbols", response_model=List[StockSymbol], tags=["Stock Data"])
@limiter.limit("30/minute")
async def get_multiple_symbols(
    request: Request,
    symbols: str,
    current_user: TokenData = Depends(get_current_user)
):
    """
    Get data for multiple symbols.
    
    - **symbols**: Comma-separated list of symbols (e.g., 1211,2222,1180)
    """
    symbol_list = [s.strip().upper() for s in symbols.split(',')]
    service = request.app.state.stock_service
    
    results = []
    for symbol in symbol_list[:10]:  # Limit to 10 symbols per request
        try:
            data = await service.get_symbol_data(symbol)
            results.append(StockSymbol(**data))
        except Exception as e:
            logger.warning(f"Failed to fetch {symbol}: {e}")
    
    return results

@app.post("/api/v1/technical/{symbol}", response_model=TechnicalIndicator, tags=["Technical Analysis"])
@limiter.limit("30/minute")
async def calculate_technical(
    request: Request,
    symbol: str,
    indicator: str = "RSI",
    period: int = 14,
    current_user: TokenData = Depends(get_current_user)
):
    """
    Calculate technical indicators for a symbol.
    
    - **symbol**: Tadawul symbol code
    - **indicator**: Technical indicator (RSI, SMA, EMA)
    - **period**: Calculation period
    """
    service = request.app.state.stock_service
    symbol_data = await service.get_symbol_data(symbol.upper())
    
    # Simplified technical calculation
    if indicator.upper() == "RSI":
        value = 65.5  # This would be calculated from actual price data
        signal = "NEUTRAL"
    elif indicator.upper() == "SMA":
        value = symbol_data.get('last_price', 0)
        signal = "NEUTRAL"
    else:
        value = 0
        signal = "NEUTRAL"
    
    return TechnicalIndicator(
        symbol=symbol.upper(),
        indicator=indicator.upper(),
        value=value,
        signal=signal
    )

# =============================================================================
# Google Sheets Integration Endpoints
# =============================================================================

@app.post("/api/v1/export/to-sheets", tags=["Export"])
@limiter.limit("10/minute")
async def export_to_sheets(
    request: Request,
    spreadsheet_id: str,
    sheet_name: str = "StockData",
    symbols: Optional[str] = None,
    current_user: TokenData = Depends(get_current_user)
):
    """
    Export stock data to Google Sheets.
    
    - **spreadsheet_id**: Google Sheets spreadsheet ID
    - **sheet_name**: Target sheet name
    - **symbols**: Optional comma-separated symbols (default: market leaders)
    """
    service = request.app.state.stock_service
    sheets_service = request.app.state.sheets_service
    
    # Determine which symbols to export
    if symbols:
        symbol_list = [s.strip().upper() for s in symbols.split(',')]
    else:
        # Default: market leaders
        symbol_list = ["1211", "2222", "1180", "4200", "2380"]
    
    # Fetch data for all symbols
    all_data = []
    for symbol in symbol_list:
        try:
            data = await service.get_symbol_data(symbol)
            
            # Convert to row format
            row = [
                data.get('symbol', ''),
                data.get('name_en', ''),
                data.get('last_price', 0),
                data.get('change', 0),
                data.get('change_percent', 0),
                data.get('volume', 0),
                datetime.utcnow().isoformat()
            ]
            all_data.append(row)
        except Exception as e:
            logger.error(f"Failed to fetch {symbol}: {e}")
            continue
    
    if not all_data:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="No data available for export"
        )
    
    # Add headers
    headers = [
        ["Symbol", "Name", "Last Price", "Change", "Change %", "Volume", "Timestamp"]
    ]
    data_to_export = headers + all_data
    
    # Update Google Sheet
    result = await sheets_service.update_sheet(spreadsheet_id, sheet_name, data_to_export)
    
    return {
        "status": "success",
        "spreadsheet_id": spreadsheet_id,
        "sheet_name": sheet_name,
        "symbols_exported": len(all_data),
        "details": result
    }

# =============================================================================
# Data Export Endpoints
# =============================================================================

@app.get("/api/v1/export/csv/{symbol}", tags=["Export"])
@limiter.limit("20/minute")
async def export_csv(
    symbol: str,
    period: str = "1d",
    current_user: TokenData = Depends(get_current_user)
):
    """
    Export symbol data as CSV.
    
    - **symbol**: Tadawul symbol code
    - **period**: Time period (1d, 1w, 1m, 3m, 1y)
    """
    # Fetch data
    service = app.state.stock_service
    data = await service.get_symbol_data(symbol.upper())
    
    # Convert to DataFrame
    df = pd.DataFrame([data])
    
    # Generate CSV
    csv_data = df.to_csv(index=False)
    
    return StreamingResponse(
        iter([csv_data]),
        media_type="text/csv",
        headers={
            "Content-Disposition": f"attachment; filename={symbol}_data_{datetime.utcnow().date()}.csv"
        }
    )

@app.get("/api/v1/export/json/{symbol}", response_model=Dict, tags=["Export"])
@limiter.limit("20/minute")
async def export_json(
    symbol: str,
    pretty: bool = False,
    current_user: TokenData = Depends(get_current_user)
):
    """
    Export symbol data as JSON.
    
    - **symbol**: Tadawul symbol code
    - **pretty**: Pretty-print JSON output
    """
    service = app.state.stock_service
    data = await service.get_symbol_data(symbol.upper())
    
    if pretty:
        return JSONResponse(
            content=data,
            media_type="application/json"
        )
    else:
        # Use orjson for faster JSON serialization
        return Response(
            content=orjson.dumps(data),
            media_type="application/json"
        )

# =============================================================================
# Batch Processing Endpoints
# =============================================================================

@app.post("/api/v1/batch/symbols", tags=["Batch Processing"])
@limiter.limit("5/minute")
async def batch_process_symbols(
    request: Request,
    symbols: List[str],
    operations: List[str] = ["price", "volume", "change"],
    current_user: TokenData = Depends(get_current_user)
):
    """
    Process multiple symbols in batch.
    
    - **symbols**: List of symbols to process
    - **operations**: List of operations to perform
    """
    service = request.app.state.stock_service
    
    results = {}
    tasks = []
    
    for symbol in symbols[:50]:  # Limit batch size
        task = service.get_symbol_data(symbol.upper())
        tasks.append(task)
    
    # Execute all tasks concurrently
    all_data = await asyncio.gather(*tasks, return_exceptions=True)
    
    for symbol, data in zip(symbols, all_data):
        if isinstance(data, Exception):
            results[symbol] = {"error": str(data)}
        else:
            # Filter based on requested operations
            filtered_data = {}
            for op in operations:
                if op in data:
                    filtered_data[op] = data[op]
            results[symbol] = filtered_data
    
    return {
        "processed": len(results),
        "timestamp": datetime.utcnow().isoformat(),
        "results": results
    }

# =============================================================================
# Error Handlers
# =============================================================================

@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    """Handle HTTP exceptions with structured logging."""
    logger.error(
        "HTTP exception",
        status_code=exc.status_code,
        detail=exc.detail,
        path=request.url.path
    )
    
    return JSONResponse(
        status_code=exc.status_code,
        content={
            "error": True,
            "code": exc.status_code,
            "message": exc.detail,
            "timestamp": datetime.utcnow().isoformat(),
            "path": request.url.path
        }
    )

@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    """Handle unexpected exceptions."""
    logger.error(
        "Unhandled exception",
        error_type=type(exc).__name__,
        error_message=str(exc),
        path=request.url.path,
        traceback=True  # structlog will capture traceback
    )
    
    API_ERRORS.labels(
        error_type="unhandled",
        endpoint=request.url.path
    ).inc()
    
    return JSONResponse(
        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
        content={
            "error": True,
            "code": 500,
            "message": "Internal server error",
            "timestamp": datetime.utcnow().isoformat(),
            "request_id": request.headers.get("X-Request-ID", "unknown")
        }
    )

# =============================================================================
# Configuration Validation
# =============================================================================

def validate_configuration():
    """Validate application configuration."""
    try:
        # Check required settings
        if not settings.secret_key or settings.secret_key == "changeme":
            logger.warning("SECRET_KEY is not set or is default value")
        
        # Check external service configurations
        if not settings.argaam_api_key:
            logger.warning("ARGAAM_API_KEY not set - some features may be limited")
        
        if not settings.google_sheets_credentials:
            logger.warning("GOOGLE_CREDENTIALS_JSON not set - Google Sheets features disabled")
        
        logger.info("Configuration validation completed")
        
    except Exception as e:
        logger.error(f"Configuration validation failed: {e}")
        raise

# =============================================================================
# Application Initialization
# =============================================================================

# Initialize settings
settings = Settings()

# Store app start time
app.state.start_time = time.time()

# Validate configuration on startup
validate_configuration()

# =============================================================================
# Main Entry Point
# =============================================================================

if __name__ == "__main__":
    import uvicorn
    
    logger.info(f"Starting {settings.app_name} on port 8000")
    
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=settings.environment == "development",
        workers=4 if settings.environment == "production" else 1,
        log_config=None,
        access_log=True
    )
