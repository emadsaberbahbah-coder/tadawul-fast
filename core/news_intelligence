# core/news_intelligence.py
# Tadawul Fast Bridge â€” News Intelligence (Sentiment + Qualitative Boost)
# Version: 0.1.0
#
# Goal:
# - Fetch recent news (headlines/snippets) for each symbol/company
# - Score sentiment [-1..+1] and confidence [0..1]
# - Provide a "news_boost" value you can add into advisor_score
#
# Safety:
# - If anything fails (network blocked, RSS unavailable), return neutral scores.
# - Avoids heavy ML deps. Uses a lexicon-based scorer (fast + stable).
#
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple
import re
import time

try:
    import httpx  # lightweight async-capable client
except Exception:
    httpx = None  # type: ignore


TT_NEWS_VERSION = "0.1.0"


# -----------------------------------------------------------------------------
# Config
# -----------------------------------------------------------------------------
DEFAULT_TIMEOUT_SECONDS = 6.0
DEFAULT_MAX_ARTICLES = 8

# You can add/adjust sources later.
# Keep these simple + stable. RSS is easiest.
DEFAULT_RSS_SOURCES: List[str] = [
    # Global market/business RSS examples (swap/add your preferred sources)
    "https://feeds.reuters.com/reuters/businessNews",
    "https://feeds.bbci.co.uk/news/business/rss.xml",
]

# Optional: If you have a dedicated news API key later, wire it here.
# For now we keep it RSS-only to avoid API keys.


# -----------------------------------------------------------------------------
# Data structures
# -----------------------------------------------------------------------------
@dataclass
class NewsArticle:
    title: str
    url: str = ""
    source: str = ""
    published_utc: Optional[str] = None
    snippet: str = ""


@dataclass
class NewsResult:
    symbol: str
    query: str
    sentiment: float  # -1..+1
    confidence: float  # 0..1
    news_boost: float  # points to add to advisor score (e.g., -5..+5)
    articles: List[NewsArticle]


# -----------------------------------------------------------------------------
# Sentiment Lexicon (simple, explainable)
# -----------------------------------------------------------------------------
_POS_WORDS = {
    "beat", "beats", "surge", "surges", "soar", "soars", "strong", "record", "growth",
    "profit", "profits", "upgrade", "upgraded", "outperform", "buy", "bullish",
    "rebound", "expansion", "wins", "win", "contract", "contracts", "award", "awarded",
    "raises", "raise", "raised", "guidance up", "higher guidance",
}
_NEG_WORDS = {
    "miss", "misses", "plunge", "plunges", "weak", "warning", "downgrade", "downgraded",
    "sell", "bearish", "lawsuit", "probe", "investigation", "fraud", "default",
    "loss", "losses", "cuts", "cut", "cutting", "layoff", "layoffs",
    "bankruptcy", "recall", "halt", "suspends", "suspended", "sanction", "sanctions",
}

# Phrases get extra weight
_NEG_PHRASES = {"profit warning", "guidance cut", "regulatory probe"}
_POS_PHRASES = {"record profit", "raises guidance", "share buyback"}


def _normalize_text(s: str) -> str:
    s = (s or "").strip().lower()
    s = re.sub(r"\s+", " ", s)
    return s


def _lexicon_sentiment_score(text: str) -> Tuple[float, float]:
    """
    Returns (sentiment, confidence)
      sentiment: -1..+1
      confidence: 0..1 based on signal strength
    """
    t = _normalize_text(text)
    if not t:
        return 0.0, 0.0

    score = 0.0
    hits = 0

    # phrase weights
    for p in _POS_PHRASES:
        if p in t:
            score += 2.0
            hits += 2
    for p in _NEG_PHRASES:
        if p in t:
            score -= 2.0
            hits += 2

    words = re.findall(r"[a-zA-Z]+", t)
    for w in words:
        if w in _POS_WORDS:
            score += 1.0
            hits += 1
        elif w in _NEG_WORDS:
            score -= 1.0
            hits += 1

    if hits == 0:
        return 0.0, 0.1  # neutral but low confidence

    # Normalize score into [-1..+1] with tanh-like clamp
    sentiment = max(-1.0, min(1.0, score / max(3.0, hits)))
    # Confidence increases with number of hits (cap at ~1)
    confidence = max(0.2, min(1.0, hits / 10.0))
    return sentiment, confidence


def _to_news_boost(sentiment: float, confidence: float) -> float:
    """
    Convert sentiment into advisor score boost.
    Range suggestion: about [-5..+5]
    """
    boost = sentiment * (2.5 + 2.5 * confidence)  # [-5..+5]
    return float(max(-5.0, min(5.0, boost)))


# -----------------------------------------------------------------------------
# RSS Fetch (simple)
# -----------------------------------------------------------------------------
async def _fetch_text(url: str, timeout_s: float) -> str:
    if httpx is None:
        raise RuntimeError("httpx is not installed/available.")
    async with httpx.AsyncClient(timeout=timeout_s, follow_redirects=True) as client:
        r = await client.get(url, headers={"User-Agent": "TadawulFastBridge/1.0"})
        r.raise_for_status()
        return r.text


def _parse_rss_items(xml_text: str, source_url: str, max_items: int) -> List[NewsArticle]:
    """
    Minimal RSS/Atom parsing using regex (safe fallback; not perfect, but stable).
    If you want richer parsing later, we can switch to feedparser.
    """
    txt = xml_text or ""
    # Try RSS <item> blocks first
    items = re.findall(r"<item\b.*?</item>", txt, flags=re.IGNORECASE | re.DOTALL)
    if not items:
        # Atom <entry>
        items = re.findall(r"<entry\b.*?</entry>", txt, flags=re.IGNORECASE | re.DOTALL)

    out: List[NewsArticle] = []
    for blk in items[:max_items]:
        title = _extract_xml_tag(blk, "title")
        link = _extract_xml_tag(blk, "link")
        if not link:
            # Atom: <link href="..."/>
            m = re.search(r'<link[^>]+href="([^"]+)"', blk, flags=re.IGNORECASE)
            link = m.group(1) if m else ""
        pub = _extract_xml_tag(blk, "pubDate") or _extract_xml_tag(blk, "updated")
        desc = _extract_xml_tag(blk, "description") or _extract_xml_tag(blk, "summary")

        out.append(
            NewsArticle(
                title=_strip_html(title),
                url=_strip_html(link),
                source=source_url,
                published_utc=_strip_html(pub) if pub else None,
                snippet=_strip_html(desc)[:240] if desc else "",
            )
        )
    return out


def _extract_xml_tag(block: str, tag: str) -> str:
    m = re.search(rf"<{tag}\b[^>]*>(.*?)</{tag}>", block, flags=re.IGNORECASE | re.DOTALL)
    return m.group(1).strip() if m else ""


def _strip_html(s: str) -> str:
    s = s or ""
    # Remove CDATA
    s = re.sub(r"<!\[CDATA\[(.*?)\]\]>", r"\1", s, flags=re.DOTALL)
    # Strip HTML tags
    s = re.sub(r"<[^>]+>", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s


def _match_relevance(articles: List[NewsArticle], query_terms: List[str]) -> List[NewsArticle]:
    """
    Filter RSS items to those containing query terms in title/snippet.
    If nothing matches, return the original top items (fallback).
    """
    q = [t.lower() for t in query_terms if t]
    if not q:
        return articles[:]

    matched: List[NewsArticle] = []
    for a in articles:
        text = f"{a.title} {a.snippet}".lower()
        if any(term in text for term in q):
            matched.append(a)

    return matched if matched else articles[:]


# -----------------------------------------------------------------------------
# Public API
# -----------------------------------------------------------------------------
async def get_news_intelligence(
    symbol: str,
    company_name: str = "",
    *,
    rss_sources: Optional[List[str]] = None,
    max_articles: int = DEFAULT_MAX_ARTICLES,
    timeout_s: float = DEFAULT_TIMEOUT_SECONDS,
) -> NewsResult:
    """
    Returns NewsResult for one symbol.
    """
    t0 = time.time()
    rss_sources = rss_sources or DEFAULT_RSS_SOURCES

    # Query terms: symbol + significant words in name
    terms = [symbol]
    if company_name:
        # keep only tokens longer than 3 chars
        terms += [w for w in re.findall(r"[A-Za-z0-9]+", company_name) if len(w) > 3]
    terms = list(dict.fromkeys([t.strip() for t in terms if t.strip()]))[:6]

    all_articles: List[NewsArticle] = []
    errors: List[str] = []

    # If httpx not available, return neutral
    if httpx is None:
        return NewsResult(
            symbol=symbol,
            query=" ".join(terms),
            sentiment=0.0,
            confidence=0.0,
            news_boost=0.0,
            articles=[],
        )

    for src in rss_sources:
        try:
            xml = await _fetch_text(src, timeout_s=timeout_s)
            arts = _parse_rss_items(xml, source_url=src, max_items=max_articles)
            arts = _match_relevance(arts, terms)
            all_articles.extend(arts)
        except Exception as exc:
            errors.append(f"{src}: {exc}")

    # De-dup by title
    seen = set()
    deduped: List[NewsArticle] = []
    for a in all_articles:
        key = _normalize_text(a.title)[:140]
        if key and key not in seen:
            seen.add(key)
            deduped.append(a)

    deduped = deduped[:max_articles]

    # Sentiment scoring using titles + snippets
    corpus = " | ".join([f"{a.title}. {a.snippet}" for a in deduped])
    sentiment, confidence = _lexicon_sentiment_score(corpus)
    boost = _to_news_boost(sentiment, confidence)

    # We keep diagnostics in articles source/published fields; caller can log errors if needed.
    _ = int((time.time() - t0) * 1000)

    return NewsResult(
        symbol=symbol,
        query=" ".join(terms),
        sentiment=sentiment,
        confidence=confidence,
        news_boost=boost,
        articles=deduped,
    )


async def batch_news_intelligence(
    items: List[Dict[str, str]],
    *,
    rss_sources: Optional[List[str]] = None,
    max_articles: int = DEFAULT_MAX_ARTICLES,
    timeout_s: float = DEFAULT_TIMEOUT_SECONDS,
) -> Dict[str, Any]:
    """
    items: [{"symbol": "...", "name": "..."}, ...]
    Returns:
      {"items": [NewsResult-as-dict...], "meta": {...}}
    """
    results: List[Dict[str, Any]] = []
    for it in items:
        sym = str(it.get("symbol", "")).strip()
        nm = str(it.get("name", "")).strip()
        if not sym:
            continue
        r = await get_news_intelligence(
            sym, nm, rss_sources=rss_sources, max_articles=max_articles, timeout_s=timeout_s
        )
        results.append(
            {
                "symbol": r.symbol,
                "query": r.query,
                "sentiment": r.sentiment,
                "confidence": r.confidence,
                "news_boost": r.news_boost,
                "articles": [
                    {
                        "title": a.title,
                        "url": a.url,
                        "source": a.source,
                        "published_utc": a.published_utc,
                        "snippet": a.snippet,
                    }
                    for a in r.articles
                ],
            }
        )

    return {
        "items": results,
        "meta": {"version": TT_NEWS_VERSION, "count": len(results)},
    }
