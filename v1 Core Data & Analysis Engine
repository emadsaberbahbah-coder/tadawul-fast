"""
Core Data & Analysis Engine - v2.0
Enhanced with multi-provider merging, intelligent caching, advanced analysis,
and configurable data quality assessment.

Author: Emad Bahbah (with GPT-5.1 Thinking)

Key Improvements:
1. Asynchronous provider calls with timeout and retry mechanisms
2. Intelligent caching with TTL and stale-while-revalidate
3. Configurable data quality scoring with field weighting
4. Advanced opportunity scoring with sector-aware analysis
5. Event-driven architecture for data updates
6. Built-in performance metrics and monitoring
7. Enhanced KSA market support
8. Data validation and sanitization

Usage:
    from core.data_engine_v2 import DataEngine
    
    engine = DataEngine()
    result = await engine.get_enriched_quote("MSFT")
    print(result.model_dump())
    
    # Or with custom configuration:
    engine = DataEngine(
        cache_ttl=300,
        provider_timeout=10,
        enable_advanced_analysis=True
    )
"""

from __future__ import annotations

import asyncio
import hashlib
import json
import logging
import math
import time
from abc import ABC, abstractmethod
from collections import defaultdict
from datetime import datetime, timedelta, timezone
from decimal import Decimal, ROUND_HALF_UP
from enum import Enum
from typing import Dict, Optional, List, Literal, Any, Set, Tuple, Callable, Awaitable
from dataclasses import dataclass, field

import aiohttp
from pydantic import BaseModel, Field, validator, root_validator
from redis import asyncio as aioredis

# ---------------------------------------------------------------------------
# 0) SETUP & CONFIGURATION
# ---------------------------------------------------------------------------

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Constants
DEFAULT_CACHE_TTL = 300  # 5 minutes
PROVIDER_TIMEOUT = 15  # seconds
MAX_RETRIES = 2
RETRY_DELAY = 1  # second

# ---------------------------------------------------------------------------
# 1) ENHANCED ENUMS AND TYPES
# ---------------------------------------------------------------------------

class DataQualityLevel(str, Enum):
    EXCELLENT = "EXCELLENT"  # 90-100% coverage
    GOOD = "GOOD"          # 75-89% coverage
    FAIR = "FAIR"          # 50-74% coverage
    POOR = "POOR"          # 25-49% coverage
    MISSING = "MISSING"    # <25% coverage

class MarketRegion(str, Enum):
    KSA = "KSA"
    US = "US"
    EU = "EU"
    ASIA = "ASIA"
    GLOBAL = "GLOBAL"
    UNKNOWN = "UNKNOWN"

class ProviderPriority(int, Enum):
    PRIMARY = 1
    SECONDARY = 2
    FALLBACK = 3

class AnalysisSignal(str, Enum):
    STRONG_BUY = "STRONG_BUY"
    BUY = "BUY"
    HOLD = "HOLD"
    SELL = "SELL"
    STRONG_SELL = "STRONG_SELL"
    UNKNOWN = "UNKNOWN"

# ---------------------------------------------------------------------------
# 2) CONFIGURATION MODELS
# ---------------------------------------------------------------------------

@dataclass
class ProviderConfig:
    """Configuration for a data provider"""
    name: str
    enabled: bool = True
    priority: ProviderPriority = ProviderPriority.SECONDARY
    timeout: int = PROVIDER_TIMEOUT
    max_retries: int = MAX_RETRIES
    cache_key_prefix: str = "provider"
    weight: float = 1.0  # Weight for weighted averaging
    
@dataclass
class CacheConfig:
    """Cache configuration"""
    enabled: bool = True
    ttl: int = DEFAULT_CACHE_TTL
    stale_while_revalidate: int = 60  # Serve stale data while refreshing
    redis_url: Optional[str] = None
    
@dataclass
class AnalysisConfig:
    """Analysis engine configuration"""
    enable_advanced_analysis: bool = True
    enable_sector_comparison: bool = True
    enable_technical_signals: bool = False
    opportunity_score_weights: Dict[str, float] = field(default_factory=lambda: {
        "valuation": 0.3,
        "profitability": 0.25,
        "growth": 0.25,
        "financial_health": 0.2
    })

# ---------------------------------------------------------------------------
# 3) ENHANCED DATA MODELS
# ---------------------------------------------------------------------------

class QuoteSourceInfo(BaseModel):
    """Enhanced source information with confidence scoring"""
    provider: str
    timestamp: datetime
    fields: List[str] = Field(default_factory=list)
    confidence: float = Field(1.0, ge=0.0, le=1.0)  # Provider confidence score
    latency_ms: Optional[float] = None
    
    class Config:
        json_encoders = {
            datetime: lambda dt: dt.isoformat()
        }

class FieldMetadata(BaseModel):
    """Metadata for individual data fields"""
    value: Any
    source: str
    timestamp: datetime
    confidence: float = 1.0
    last_verified: Optional[datetime] = None
    
    @validator('confidence')
    def validate_confidence(cls, v):
        return max(0.0, min(1.0, v))

class UnifiedQuote(BaseModel):
    """
    Enhanced unified quote with metadata for each field
    """
    # Basic Identification
    symbol: str
    name: Optional[str] = None
    exchange: Optional[str] = None
    currency: Optional[str] = None
    market_region: MarketRegion = MarketRegion.UNKNOWN
    sector: Optional[str] = None
    industry: Optional[str] = None
    
    # Price Data
    price: Optional[float] = None
    prev_close: Optional[float] = None
    open: Optional[float] = None
    high: Optional[float] = None
    low: Optional[float] = None
    volume: Optional[int] = None
    avg_volume_30d: Optional[int] = None
    
    # Derived Price Metrics
    change: Optional[float] = None
    change_pct: Optional[float] = None
    fifty_two_week_high: Optional[float] = None
    fifty_two_week_low: Optional[float] = None
    ytd_return: Optional[float] = None
    
    # Valuation Metrics
    market_cap: Optional[float] = None
    enterprise_value: Optional[float] = None
    eps_ttm: Optional[float] = None
    eps_forward: Optional[float] = None
    pe_ttm: Optional[float] = None
    pe_forward: Optional[float] = None
    pb: Optional[float] = None
    ps: Optional[float] = None
    ev_to_ebitda: Optional[float] = None
    ev_to_revenue: Optional[float] = None
    dividend_yield: Optional[float] = None
    dividend_payout_ratio: Optional[float] = None
    
    # Profitability Metrics
    roe: Optional[float] = None
    roa: Optional[float] = None
    roic: Optional[float] = None
    gross_margin: Optional[float] = None
    operating_margin: Optional[float] = None
    profit_margin: Optional[float] = None
    ebitda_margin: Optional[float] = None
    
    # Growth Metrics
    revenue_growth_yoy: Optional[float] = None
    revenue_growth_3y: Optional[float] = None
    eps_growth_yoy: Optional[float] = None
    eps_growth_3y: Optional[float] = None
    fcf_growth_yoy: Optional[float] = None
    
    # Financial Health
    debt_to_equity: Optional[float] = None
    current_ratio: Optional[float] = None
    quick_ratio: Optional[float] = None
    interest_coverage: Optional[float] = None
    fcf_yield: Optional[float] = None
    net_debt_to_ebitda: Optional[float] = None
    
    # Efficiency Metrics
    asset_turnover: Optional[float] = None
    inventory_turnover: Optional[float] = None
    receivables_turnover: Optional[float] = None
    
    # Technical Indicators (if enabled)
    rsi_14: Optional[float] = None
    macd: Optional[float] = None
    bollinger_position: Optional[float] = None
    
    # Meta Information
    last_updated_utc: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    data_quality: DataQualityLevel = DataQualityLevel.MISSING
    data_quality_score: float = Field(0.0, ge=0.0, le=1.0)
    data_gaps: List[str] = Field(default_factory=list)
    sources: List[QuoteSourceInfo] = Field(default_factory=list)
    field_metadata: Dict[str, FieldMetadata] = Field(default_factory=dict)
    
    # Analysis Results
    opportunity_score: Optional[float] = Field(None, ge=0.0, le=100.0)
    risk_score: Optional[float] = Field(None, ge=0.0, le=100.0)
    value_score: Optional[float] = Field(None, ge=0.0, le=100.0)
    growth_score: Optional[float] = Field(None, ge=0.0, le=100.0)
    quality_score: Optional[float] = Field(None, ge=0.0, le=100.0)
    
    # Signals
    primary_signal: AnalysisSignal = AnalysisSignal.UNKNOWN
    secondary_signals: List[AnalysisSignal] = Field(default_factory=list)
    
    # Risk Assessment
    risk_flags: List[str] = Field(default_factory=list)
    warning_flags: List[str] = Field(default_factory=list)
    
    # Enhanced Analysis
    sector_percentile: Dict[str, float] = Field(default_factory=dict)
    historical_consistency: Optional[float] = None
    analyst_consensus: Optional[str] = None
    target_price: Optional[float] = None
    upside_potential: Optional[float] = None
    
    # Performance Metrics
    processing_time_ms: Optional[float] = None
    cache_status: Optional[str] = None
    
    # Recommendations
    summary: Optional[str] = None
    key_strengths: List[str] = Field(default_factory=list)
    key_concerns: List[str] = Field(default_factory=list)
    recommendations: List[str] = Field(default_factory=list)
    
    class Config:
        json_encoders = {
            datetime: lambda dt: dt.isoformat(),
            Decimal: lambda d: float(d.quantize(Decimal('0.01'), rounding=ROUND_HALF_UP))
        }
    
    @validator('price', 'prev_close', 'open', 'high', 'low')
    def validate_price_positive(cls, v):
        if v is not None and v <= 0:
            raise ValueError(f"Price must be positive: {v}")
        return v
    
    @root_validator
    def calculate_derived_fields(cls, values):
        """Calculate derived fields automatically"""
        price = values.get('price')
        prev_close = values.get('prev_close')
        
        # Calculate change and change percentage
        if price is not None and prev_close is not None and prev_close > 0:
            values['change'] = price - prev_close
            values['change_pct'] = ((price - prev_close) / prev_close) * 100
            
        # Calculate upside potential if target price exists
        target_price = values.get('target_price')
        if target_price is not None and price is not None and price > 0:
            values['upside_potential'] = ((target_price - price) / price) * 100
            
        return values
    
    def get_field_sources(self) -> Dict[str, List[str]]:
        """Get sources for each field"""
        result = defaultdict(list)
        for field_name, metadata in self.field_metadata.items():
            result[field_name].append(metadata.source)
        return dict(result)

# ---------------------------------------------------------------------------
# 4) PROVIDER ABSTRACTION LAYER
# ---------------------------------------------------------------------------

class DataProvider(ABC):
    """Abstract base class for data providers"""
    
    def __init__(self, config: ProviderConfig):
        self.config = config
        self.session: Optional[aiohttp.ClientSession] = None
        self._stats = {
            'calls': 0,
            'errors': 0,
            'avg_latency': 0.0,
            'last_call': None
        }
    
    async def __aenter__(self):
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=self.config.timeout)
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    @abstractmethod
    async def fetch_quote(self, symbol: str) -> Dict[str, Any]:
        """Fetch quote data for given symbol"""
        pass
    
    @abstractmethod
    def normalize_data(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """Normalize provider-specific data to unified format"""
        pass
    
    async def fetch_with_retry(self, symbol: str) -> Optional[Dict[str, Any]]:
        """Fetch data with retry logic"""
        start_time = time.time()
        
        for attempt in range(self.config.max_retries + 1):
            try:
                if self.session is None:
                    async with aiohttp.ClientSession(
                        timeout=aiohttp.ClientTimeout(total=self.config.timeout)
                    ) as temp_session:
                        self.session = temp_session
                        raw_data = await self.fetch_quote(symbol)
                else:
                    raw_data = await self.fetch_quote(symbol)
                
                # Update statistics
                latency = (time.time() - start_time) * 1000
                self._stats['calls'] += 1
                self._stats['avg_latency'] = (
                    (self._stats['avg_latency'] * (self._stats['calls'] - 1) + latency) 
                    / self._stats['calls']
                )
                self._stats['last_call'] = datetime.now(timezone.utc)
                
                return self.normalize_data(raw_data)
                
            except asyncio.TimeoutError:
                logger.warning(f"Timeout fetching {symbol} from {self.config.name} (attempt {attempt + 1})")
                if attempt < self.config.max_retries:
                    await asyncio.sleep(RETRY_DELAY * (2 ** attempt))
            except Exception as e:
                logger.error(f"Error fetching from {self.config.name}: {str(e)}")
                self._stats['errors'] += 1
                if attempt < self.config.max_retries:
                    await asyncio.sleep(RETRY_DELAY * (2 ** attempt))
        
        return None
    
    def get_stats(self) -> Dict[str, Any]:
        """Get provider statistics"""
        return self._stats.copy()

# Concrete Provider Implementations

class EODHDProvider(DataProvider):
    """EODHD API provider"""
    
    def __init__(self, api_key: Optional[str] = None):
        config = ProviderConfig(
            name="EODHD",
            priority=ProviderPriority.PRIMARY,
            weight=1.0
        )
        super().__init__(config)
        self.api_key = api_key or "demo"  # In production, load from config
    
    async def fetch_quote(self, symbol: str) -> Dict[str, Any]:
        """Fetch data from EODHD API"""
        # Real implementation would make actual API calls
        # This is a stub for illustration
        
        # Example endpoints:
        # /api/real-time/{symbol}?api_token={api_key}
        # /api/fundamentals/{symbol}?api_token={api_key}
        
        url = f"https://eodhistoricaldata.com/api/real-time/{symbol}"
        params = {
            'api_token': self.api_key,
            'fmt': 'json'
        }
        
        async with self.session.get(url, params=params) as response:
            if response.status == 200:
                return await response.json()
            else:
                response.raise_for_status()
    
    def normalize_data(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """Normalize EODHD data to unified format"""
        normalized = {
            'symbol': raw_data.get('code', ''),
            'name': raw_data.get('name'),
            'exchange': raw_data.get('exchange'),
            'currency': raw_data.get('currency'),
            'price': raw_data.get('close'),
            'prev_close': raw_data.get('previousClose'),
            'open': raw_data.get('open'),
            'high': raw_data.get('high'),
            'low': raw_data.get('low'),
            'volume': raw_data.get('volume'),
            'market_cap': raw_data.get('marketCapitalization'),
            'timestamp': datetime.now(timezone.utc)
        }
        
        # Clean None values
        return {k: v for k, v in normalized.items() if v is not None}

class FMPProvider(DataProvider):
    """Financial Modeling Prep provider"""
    
    def __init__(self, api_key: Optional[str] = None):
        config = ProviderConfig(
            name="FMP",
            priority=ProviderPriority.SECONDARY,
            weight=0.9
        )
        super().__init__(config)
        self.api_key = api_key
    
    async def fetch_quote(self, symbol: str) -> Dict[str, Any]:
        """Fetch data from FMP API"""
        # Stub implementation
        return {}
    
    def normalize_data(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """Normalize FMP data"""
        return {}

class TadawulBridgeProvider(DataProvider):
    """Tadawul Bridge provider for KSA stocks"""
    
    def __init__(self, base_url: Optional[str] = None):
        config = ProviderConfig(
            name="TADAWUL_BRIDGE",
            priority=ProviderPriority.PRIMARY,
            weight=1.0
        )
        super().__init__(config)
        self.base_url = base_url or "http://localhost:8000"  # Your tadawul bridge URL
    
    async def fetch_quote(self, symbol: str) -> Dict[str, Any]:
        """Fetch data from Tadawul Bridge"""
        if not symbol.endswith('.SR'):
            return {}
        
        url = f"{self.base_url}/api/quote/{symbol}"
        async with self.session.get(url) as response:
            if response.status == 200:
                return await response.json()
            else:
                response.raise_for_status()
    
    def normalize_data(self, raw_data: Dict[str, Any]) -> Dict[str, Any]:
        """Normalize Tadawul data"""
        normalized = {
            'symbol': raw_data.get('symbol'),
            'name': raw_data.get('name_ar'),  # Arabic name
            'name_en': raw_data.get('name_en'),  # English name
            'exchange': 'TADAWUL',
            'currency': 'SAR',
            'price': raw_data.get('current_price'),
            'prev_close': raw_data.get('previous_close'),
            'open': raw_data.get('open_price'),
            'high': raw_data.get('high_price'),
            'low': raw_data.get('low_price'),
            'volume': raw_data.get('volume'),
            'market_cap': raw_data.get('market_cap'),
            'market_region': MarketRegion.KSA,
            'timestamp': datetime.now(timezone.utc)
        }
        
        # Add KSA-specific fields
        if 'sector' in raw_data:
            normalized['sector'] = raw_data['sector']
        
        return {k: v for k, v in normalized.items() if v is not None}

# ---------------------------------------------------------------------------
# 5) INTELLIGENT MERGING ENGINE
# ---------------------------------------------------------------------------

class DataMerger:
    """Intelligent data merging engine"""
    
    def __init__(self, field_weights: Optional[Dict[str, float]] = None):
        self.field_weights = field_weights or self._get_default_weights()
    
    @staticmethod
    def _get_default_weights() -> Dict[str, float]:
        """Get default field weights for merging"""
        return {
            # Price data (highest priority)
            'price': 1.0,
            'prev_close': 0.9,
            'open': 0.8,
            'high': 0.8,
            'low': 0.8,
            'volume': 0.7,
            
            # Fundamentals
            'market_cap': 0.9,
            'eps_ttm': 0.8,
            'pe_ttm': 0.8,
            'pb': 0.7,
            
            # Company info
            'name': 0.6,
            'exchange': 0.5,
            'currency': 0.5,
        }
    
    def merge_provider_data(
        self,
        symbol: str,
        provider_data: List[Tuple[str, Dict[str, Any], QuoteSourceInfo]]
    ) -> Dict[str, Any]:
        """
        Merge data from multiple providers intelligently
        
        Args:
            symbol: Stock symbol
            provider_data: List of (provider_name, data, source_info) tuples
            
        Returns:
            Merged data dictionary
        """
        if not provider_data:
            return {'symbol': symbol, 'data_quality': DataQualityLevel.MISSING}
        
        # Group data by field
        field_values = defaultdict(list)
        field_sources = defaultdict(list)
        field_confidences = defaultdict(list)
        field_timestamps = defaultdict(list)
        
        for provider_name, data, source_info in provider_data:
            for field, value in data.items():
                if value is not None:
                    field_values[field].append(value)
                    field_sources[field].append(provider_name)
                    field_confidences[field].append(source_info.confidence)
                    field_timestamps[field].append(source_info.timestamp)
        
        # Merge each field
        merged = {'symbol': symbol}
        metadata = {}
        
        for field, values in field_values.items():
            if not values:
                continue
                
            # Choose merging strategy based on field type
            if field in ['price', 'prev_close', 'open', 'high', 'low']:
                # For prices, use weighted average based on confidence
                merged_value = self._weighted_average(
                    values, 
                    field_confidences[field]
                )
            elif field in ['volume', 'market_cap']:
                # For these, use median to avoid outliers
                merged_value = self._median(values)
            elif field in ['name', 'exchange', 'currency', 'sector']:
                # For text fields, use most frequent or highest confidence
                merged_value = self._most_confident(
                    values, field_confidences[field]
                )
            else:
                # For other fields, use average
                merged_value = self._weighted_average(
                    values, field_confidences[field]
                )
            
            merged[field] = merged_value
            
            # Store metadata
            latest_timestamp = max(field_timestamps[field])
            avg_confidence = sum(field_confidences[field]) / len(field_confidences[field])
            
            metadata[field] = FieldMetadata(
                value=merged_value,
                source=field_sources[field][0],  # Primary source
                timestamp=latest_timestamp,
                confidence=avg_confidence,
                last_verified=latest_timestamp
            )
        
        # Infer market region
        merged['market_region'] = self._infer_market_region(
            merged.get('symbol') or symbol,
            merged.get('exchange')
        )
        
        # Add metadata
        merged['field_metadata'] = metadata
        
        return merged
    
    @staticmethod
    def _weighted_average(values: List[Any], weights: List[float]) -> float:
        """Calculate weighted average"""
        if not values:
            return 0.0
        
        weighted_sum = sum(v * w for v, w in zip(values, weights))
        total_weight = sum(weights)
        
        return weighted_sum / total_weight if total_weight > 0 else sum(values) / len(values)
    
    @staticmethod
    def _median(values: List[Any]) -> float:
        """Calculate median"""
        sorted_values = sorted(values)
        n = len(sorted_values)
        
        if n % 2 == 1:
            return sorted_values[n // 2]
        else:
            mid = n // 2
            return (sorted_values[mid - 1] + sorted_values[mid]) / 2
    
    @staticmethod
    def _most_confident(values: List[Any], confidences: List[float]) -> Any:
        """Return value with highest confidence"""
        if not values:
            return None
        
        max_idx = confidences.index(max(confidences))
        return values[max_idx]
    
    @staticmethod
    def _infer_market_region(symbol: str, exchange: Optional[str]) -> MarketRegion:
        """Infer market region from symbol and exchange"""
        symbol_upper = symbol.upper()
        
        # KSA stocks
        if symbol_upper.endswith('.SR') or (exchange and exchange.upper() == 'TADAWUL'):
            return MarketRegion.KSA
        
        # US stocks
        us_exchanges = {'NASDAQ', 'NYSE', 'AMEX'}
        if exchange and exchange.upper() in us_exchanges:
            return MarketRegion.US
        
        # European stocks
        eu_exchanges = {'LSE', 'XETRA', 'EURONEXT', 'SIX'}
        if exchange and exchange.upper() in eu_exchanges:
            return MarketRegion.EU
        
        # Asian stocks
        asia_exchanges = {'TSE', 'HKEX', 'SSE', 'SZSE'}
        if exchange and exchange.upper() in asia_exchanges:
            return MarketRegion.ASIA
        
        return MarketRegion.UNKNOWN

# ---------------------------------------------------------------------------
# 6) ADVANCED ANALYSIS ENGINE
# ---------------------------------------------------------------------------

class AnalysisEngine:
    """Advanced analysis engine with multiple scoring models"""
    
    def __init__(self, config: Optional[AnalysisConfig] = None):
        self.config = config or AnalysisConfig()
        self.sector_data = self._load_sector_data()
    
    def _load_sector_data(self) -> Dict[str, Dict[str, float]]:
        """Load sector benchmark data"""
        # In production, load from database or API
        return {
            "Technology": {
                "avg_pe": 25.0,
                "avg_roe": 0.20,
                "avg_profit_margin": 0.15,
                "avg_revenue_growth": 0.12
            },
            "Financial Services": {
                "avg_pe": 12.0,
                "avg_roe": 0.15,
                "avg_profit_margin": 0.25,
                "avg_revenue_growth": 0.08
            },
            "Healthcare": {
                "avg_pe": 22.0,
                "avg_roe": 0.18,
                "avg_profit_margin": 0.12,
                "avg_revenue_growth": 0.10
            },
            # Add more sectors as needed
        }
    
    def analyze_quote(self, quote_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Perform comprehensive analysis on quote data
        
        Returns:
            Dictionary with analysis results
        """
        analysis = {}
        
        # Calculate individual scores
        analysis['value_score'] = self._calculate_value_score(quote_data)
        analysis['growth_score'] = self._calculate_growth_score(quote_data)
        analysis['quality_score'] = self._calculate_quality_score(quote_data)
        analysis['risk_score'] = self._calculate_risk_score(quote_data)
        
        # Calculate composite opportunity score
        analysis['opportunity_score'] = self._calculate_opportunity_score(
            analysis, quote_data
        )
        
        # Generate signals
        analysis['primary_signal'] = self._generate_primary_signal(
            analysis['opportunity_score'],
            quote_data
        )
        
        analysis['secondary_signals'] = self._generate_secondary_signals(
            analysis, quote_data
        )
        
        # Identify risk flags
        analysis['risk_flags'] = self._identify_risk_flags(quote_data)
        analysis['warning_flags'] = self._identify_warning_flags(quote_data)
        
        # Calculate sector percentiles
        if self.config.enable_sector_comparison:
            analysis['sector_percentile'] = self._calculate_sector_percentiles(
                quote_data
            )
        
        # Generate summary and recommendations
        analysis.update(self._generate_recommendations(analysis, quote_data))
        
        return analysis
    
    def _calculate_value_score(self, data: Dict[str, Any]) -> float:
        """Calculate value investment score (0-100)"""
        score = 50.0  # Neutral baseline
        
        # P/E ratio scoring
        pe = data.get('pe_ttm')
        if pe is not None:
            if pe < 10:
                score += 20
            elif pe < 15:
                score += 10
            elif pe > 30:
                score -= 10
            elif pe > 50:
                score -= 20
        
        # P/B ratio scoring
        pb = data.get('pb')
        if pb is not None:
            if pb < 1:
                score += 15
            elif pb < 2:
                score += 5
            elif pb > 5:
                score -= 10
        
        # Dividend yield scoring
        dividend_yield = data.get('dividend_yield')
        if dividend_yield is not None:
            if dividend_yield > 0.04:  # >4%
                score += 10
            elif dividend_yield > 0.02:  # >2%
                score += 5
        
        # FCF yield scoring
        fcf_yield = data.get('fcf_yield')
        if fcf_yield is not None and fcf_yield > 0.05:  # >5%
            score += 10
        
        return max(0.0, min(100.0, score))
    
    def _calculate_growth_score(self, data: Dict[str, Any]) -> float:
        """Calculate growth potential score (0-100)"""
        score = 50.0
        
        # Revenue growth
        rev_growth = data.get('revenue_growth_yoy')
        if rev_growth is not None:
            rev_growth_pct = rev_growth * 100
            if rev_growth_pct > 20:
                score += 20
            elif rev_growth_pct > 10:
                score += 10
            elif rev_growth_pct < 0:
                score -= 10
        
        # EPS growth
        eps_growth = data.get('eps_growth_yoy')
        if eps_growth is not None:
            eps_growth_pct = eps_growth * 100
            if eps_growth_pct > 20:
                score += 15
            elif eps_growth_pct > 10:
                score += 8
            elif eps_growth_pct < 0:
                score -= 8
        
        # Forward P/E vs TTM P/E
        pe_ttm = data.get('pe_ttm')
        pe_forward = data.get('pe_forward')
        if pe_ttm is not None and pe_forward is not None and pe_ttm > 0:
            if pe_forward < pe_ttm * 0.9:  # Forward P/E is significantly lower
                score += 10
        
        return max(0.0, min(100.0, score))
    
    def _calculate_quality_score(self, data: Dict[str, Any]) -> float:
        """Calculate business quality score (0-100)"""
        score = 50.0
        
        # Profitability metrics
        roe = data.get('roe')
        if roe is not None:
            roe_pct = roe * 100
            if roe_pct > 20:
                score += 15
            elif roe_pct > 15:
                score += 8
            elif roe_pct < 5:
                score -= 10
        
        profit_margin = data.get('profit_margin')
        if profit_margin is not None:
            margin_pct = profit_margin * 100
            if margin_pct > 20:
                score += 10
            elif margin_pct > 10:
                score += 5
            elif margin_pct < 5:
                score -= 5
        
        # Financial health
        debt_to_equity = data.get('debt_to_equity')
        if debt_to_equity is not None:
            if debt_to_equity < 0.5:
                score += 10
            elif debt_to_equity > 1.5:
                score -= 10
        
        current_ratio = data.get('current_ratio')
        if current_ratio is not None:
            if current_ratio > 2.0:
                score += 5
            elif current_ratio < 1.0:
                score -= 10
        
        return max(0.0, min(100.0, score))
    
    def _calculate_risk_score(self, data: Dict[str, Any]) -> float:
        """Calculate risk score (0-100, lower is better)"""
        score = 50.0  # Neutral baseline
        
        # Volatility indicators
        beta = data.get('beta')  # Would need to add this field
        if beta is not None:
            if beta > 1.5:
                score += 20  # Higher risk
            elif beta < 0.8:
                score -= 10  # Lower risk
        
        # Debt levels
        debt_to_equity = data.get('debt_to_equity')
        if debt_to_equity is not None:
            if debt_to_equity > 2.0:
                score += 15
            elif debt_to_equity > 1.0:
                score += 5
        
        # Profitability consistency
        if data.get('revenue_growth_yoy') is not None and data.get('revenue_growth_yoy') < 0:
            score += 10
        
        if data.get('profit_margin') is not None and data.get('profit_margin') < 0.05:
            score += 5
        
        return max(0.0, min(100.0, score))
    
    def _calculate_opportunity_score(
        self, 
        individual_scores: Dict[str, float],
        data: Dict[str, Any]
    ) -> float:
        """Calculate composite opportunity score"""
        weights = self.config.opportunity_score_weights
        
        # Weighted average of component scores
        weighted_sum = 0
        total_weight = 0
        
        for score_name, weight in weights.items():
            if score_name == 'valuation':
                score = individual_scores.get('value_score', 50)
            elif score_name == 'profitability':
                score = individual_scores.get('quality_score', 50)
            elif score_name == 'growth':
                score = individual_scores.get('growth_score', 50)
            elif score_name == 'financial_health':
                score = 100 - individual_scores.get('risk_score', 50)  # Inverse of risk
            else:
                continue
                
            weighted_sum += score * weight
            total_weight += weight
        
        if total_weight > 0:
            base_score = weighted_sum / total_weight
        else:
            base_score = 50.0
        
        # Apply sector adjustment
        if self.config.enable_sector_comparison:
            sector_adjustment = self._calculate_sector_adjustment(data)
            base_score += sector_adjustment
        
        return max(0.0, min(100.0, base_score))
    
    def _calculate_sector_adjustment(self, data: Dict[str, Any]) -> float:
        """Adjust score based on sector performance"""
        sector = data.get('sector')
        if not sector or sector not in self.sector_data:
            return 0.0
        
        sector_benchmarks = self.sector_data[sector]
        adjustment = 0.0
        
        # Compare P/E to sector average
        pe = data.get('pe_ttm')
        sector_pe = sector_benchmarks.get('avg_pe')
        if pe is not None and sector_pe is not None:
            if pe < sector_pe * 0.8:  # Undervalued relative to sector
                adjustment += 5
            elif pe > sector_pe * 1.2:  # Overvalued relative to sector
                adjustment -= 5
        
        # Compare ROE to sector average
        roe = data.get('roe')
        sector_roe = sector_benchmarks.get('avg_roe')
        if roe is not None and sector_roe is not None:
            if roe > sector_roe * 1.2:  # Better profitability
                adjustment += 5
            elif roe < sector_roe * 0.8:  # Worse profitability
                adjustment -= 5
        
        return adjustment
    
    def _generate_primary_signal(
        self, 
        opportunity_score: float,
        data: Dict[str, Any]
    ) -> AnalysisSignal:
        """Generate primary investment signal"""
        if opportunity_score >= 80:
            return AnalysisSignal.STRONG_BUY
        elif opportunity_score >= 65:
            return AnalysisSignal.BUY
        elif opportunity_score >= 40:
            return AnalysisSignal.HOLD
        elif opportunity_score >= 20:
            return AnalysisSignal.SELL
        else:
            return AnalysisSignal.STRONG_SELL
    
    def _generate_secondary_signals(
        self,
        analysis: Dict[str, Any],
        data: Dict[str, Any]
    ) -> List[AnalysisSignal]:
        """Generate secondary signals based on individual scores"""
        signals = []
        
        # Value signal
        if analysis.get('value_score', 50) >= 70:
            signals.append(AnalysisSignal.BUY)
        elif analysis.get('value_score', 50) <= 30:
            signals.append(AnalysisSignal.SELL)
        
        # Growth signal
        if analysis.get('growth_score', 50) >= 70:
            signals.append(AnalysisSignal.BUY)
        
        # Quality signal
        if analysis.get('quality_score', 50) >= 70:
            signals.append(AnalysisSignal.BUY)
        
        return list(set(signals))  # Remove duplicates
    
    def _identify_risk_flags(self, data: Dict[str, Any]) -> List[str]:
        """Identify critical risk flags"""
        flags = []
        
        # High debt
        debt_to_equity = data.get('debt_to_equity')
        if debt_to_equity is not None and debt_to_equity > 2.0:
            flags.append("HIGH_DEBT_LEVEL")
        
        # Negative profitability
        profit_margin = data.get('profit_margin')
        if profit_margin is not None and profit_margin < 0:
            flags.append("NEGATIVE_PROFITABILITY")
        
        # Declining revenue
        revenue_growth = data.get('revenue_growth_yoy')
        if revenue_growth is not None and revenue_growth < -0.1:  # >10% decline
            flags.append("REVENUE_DECLINE")
        
        # Low liquidity
        current_ratio = data.get('current_ratio')
        if current_ratio is not None and current_ratio < 1.0:
            flags.append("LIQUIDITY_CONCERN")
        
        return flags
    
    def _identify_warning_flags(self, data: Dict[str, Any]) -> List[str]:
        """Identify warning flags (less severe than risk flags)"""
        warnings = []
        
        # High valuation
        pe = data.get('pe_ttm')
        if pe is not None and pe > 40:
            warnings.append("HIGH_VALUATION")
        
        # Slowing growth
        revenue_growth = data.get('revenue_growth_yoy')
        if revenue_growth is not None and 0 > revenue_growth > -0.05:
            warnings.append("SLOWING_GROWTH")
        
        # Thin margin
        profit_margin = data.get('profit_margin')
        if profit_margin is not None and 0 < profit_margin < 0.05:
            warnings.append("THIN_MARGINS")
        
        return warnings
    
    def _calculate_sector_percentiles(self, data: Dict[str, Any]) -> Dict[str, float]:
        """Calculate percentiles relative to sector"""
        sector = data.get('sector')
        if not sector or sector not in self.sector_data:
            return {}
        
        percentiles = {}
        sector_data = self.sector_data[sector]
        
        # PE percentile
        pe = data.get('pe_ttm')
        if pe is not None:
            sector_pe = sector_data.get('avg_pe', 20)
            percentiles['pe'] = min(100, max(0, (pe / sector_pe) * 100))
        
        # ROE percentile
        roe = data.get('roe')
        if roe is not None:
            sector_roe = sector_data.get('avg_roe', 0.15)
            percentiles['roe'] = min(100, max(0, (roe / sector_roe) * 100))
        
        return percentiles
    
    def _generate_recommendations(
        self,
        analysis: Dict[str, Any],
        data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Generate summary and recommendations"""
        result = {
            'summary': '',
            'key_strengths': [],
            'key_concerns': [],
            'recommendations': []
        }
        
        opportunity_score = analysis.get('opportunity_score', 50)
        
        # Generate summary based on opportunity score
        if opportunity_score >= 75:
            result['summary'] = "Strong investment opportunity with favorable valuation and growth prospects."
        elif opportunity_score >= 60:
            result['summary'] = "Good investment opportunity with balanced risk-reward profile."
        elif opportunity_score >= 40:
            result['summary'] = "Neutral outlook. Monitor for better entry point or improved fundamentals."
        elif opportunity_score >= 25:
            result['summary'] = "Caution advised. Multiple risk factors present."
        else:
            result['summary'] = "High risk profile. Consider avoiding or short-term trading only."
        
        # Identify strengths
        if analysis.get('value_score', 50) >= 65:
            result['key_strengths'].append("Attractive valuation metrics")
        
        if analysis.get('growth_score', 50) >= 65:
            result['key_strengths'].append("Strong growth trajectory")
        
        if analysis.get('quality_score', 50) >= 65:
            result['key_strengths'].append("High business quality and profitability")
        
        if analysis.get('risk_score', 50) <= 35:
            result['key_strengths'].append("Strong financial health")
        
        # Identify concerns
        risk_flags = analysis.get('risk_flags', [])
        if risk_flags:
            result['key_concerns'].extend(risk_flags)
        
        warning_flags = analysis.get('warning_flags', [])
        if warning_flags:
            result['key_concerns'].extend(warning_flags)
        
        # Generate recommendations
        if opportunity_score >= 70:
            result['recommendations'].append("Consider accumulating position")
            result['recommendations'].append("Monitor quarterly results")
        elif opportunity_score >= 50:
            result['recommendations'].append("Hold existing position")
            result['recommendations'].append("Watch for pullbacks to add")
        else:
            result['recommendations'].append("Consider reducing exposure")
            result['recommendations'].append("Set strict stop-loss if trading")
        
        # Add sector-specific recommendations
        sector = data.get('sector')
        if sector:
            result['recommendations'].append(f"Compare with {sector} sector peers")
        
        return result

# ---------------------------------------------------------------------------
# 7) CACHE MANAGER
# ---------------------------------------------------------------------------

class CacheManager:
    """Intelligent cache manager with Redis support"""
    
    def __init__(self, config: CacheConfig):
        self.config = config
        self.redis = None
        self.local_cache = {}
        self.local_cache_timestamps = {}
        
    async def initialize(self):
        """Initialize cache connection"""
        if self.config.enabled and self.config.redis_url:
            try:
                self.redis = await aioredis.from_url(
                    self.config.redis_url,
                    decode_responses=True
                )
                logger.info("Redis cache initialized")
            except Exception as e:
                logger.warning(f"Failed to connect to Redis: {e}. Using local cache only.")
    
    async def get(self, key: str) -> Optional[Any]:
        """Get value from cache"""
        if not self.config.enabled:
            return None
        
        # Try Redis first
        if self.redis:
            try:
                cached = await self.redis.get(key)
                if cached:
                    return json.loads(cached)
            except Exception as e:
                logger.warning(f"Redis get error: {e}")
        
        # Fallback to local cache
        if key in self.local_cache:
            timestamp = self.local_cache_timestamps.get(key)
            if timestamp and (datetime.now(timezone.utc) - timestamp).seconds < self.config.ttl:
                return self.local_cache[key]
        
        return None
    
    async def set(self, key: str, value: Any, ttl: Optional[int] = None):
        """Set value in cache"""
        if not self.config.enabled:
            return
        
        ttl = ttl or self.config.ttl
        
        # Store in Redis
        if self.redis:
            try:
                await self.redis.setex(
                    key,
                    ttl,
                    json.dumps(value, default=str)
                )
            except Exception as e:
                logger.warning(f"Redis set error: {e}")
        
        # Also store in local cache
        self.local_cache[key] = value
        self.local_cache_timestamps[key] = datetime.now(timezone.utc)
    
    async def delete(self, key: str):
        """Delete value from cache"""
        if self.redis:
            try:
                await self.redis.delete(key)
            except Exception:
                pass
        
        if key in self.local_cache:
            del self.local_cache[key]
            del self.local_cache_timestamps[key]
    
    def generate_cache_key(self, symbol: str, data_type: str = "quote") -> str:
        """Generate cache key for symbol"""
        key_data = f"{data_type}:{symbol.upper()}"
        return hashlib.md5(key_data.encode()).hexdigest()

# ---------------------------------------------------------------------------
# 8) MAIN DATA ENGINE
# ---------------------------------------------------------------------------

class DataEngine:
    """
    Main data engine orchestrating providers, caching, merging, and analysis
    """
    
    def __init__(
        self,
        cache_config: Optional[CacheConfig] = None,
        analysis_config: Optional[AnalysisConfig] = None,
        enable_providers: Optional[List[str]] = None
    ):
        self.cache_config = cache_config or CacheConfig()
        self.analysis_config = analysis_config or AnalysisConfig()
        
        # Initialize components
        self.cache_manager = CacheManager(self.cache_config)
        self.data_merger = DataMerger()
        self.analysis_engine = AnalysisEngine(self.analysis_config)
        
        # Initialize providers
        self.providers = self._initialize_providers(enable_providers)
        
        # Statistics
        self.stats = {
            'total_requests': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'avg_processing_time': 0.0,
            'errors': 0
        }
    
    def _initialize_providers(
        self, 
        enable_providers: Optional[List[str]] = None
    ) -> List[DataProvider]:
        """Initialize data providers"""
        providers = []
        
        provider_configs = {
            'eodhd': EODHDProvider,
            'fmp': FMPProvider,
            'tadawul': TadawulBridgeProvider
        }
        
        enabled = enable_providers or list(provider_configs.keys())
        
        for provider_name in enabled:
            if provider_name in provider_configs:
                try:
                    provider_class = provider_configs[provider_name]
                    provider = provider_class()
                    providers.append(provider)
                    logger.info(f"Initialized provider: {provider_name}")
                except Exception as e:
                    logger.error(f"Failed to initialize {provider_name}: {e}")
        
        if not providers:
            logger.warning("No providers initialized!")
        
        return providers
    
    async def initialize(self):
        """Initialize engine components"""
        await self.cache_manager.initialize()
        logger.info("Data Engine initialized")
    
    async def get_enriched_quote(
        self, 
        symbol: str,
        use_cache: bool = True,
        force_refresh: bool = False
    ) -> UnifiedQuote:
        """
        Main entry point: get enriched quote with analysis
        
        Args:
            symbol: Stock symbol
            use_cache: Whether to use cache
            force_refresh: Force refresh even if cached
            
        Returns:
            UnifiedQuote object with full analysis
        """
        start_time = time.time()
        self.stats['total_requests'] += 1
        
        # Generate cache key
        cache_key = self.cache_manager.generate_cache_key(symbol)
        
        # Try cache first
        cached_result = None
        if use_cache and not force_refresh:
            cached_result = await self.cache_manager.get(cache_key)
            if cached_result:
                self.stats['cache_hits'] += 1
                logger.info(f"Cache hit for {symbol}")
                
                # Check if cache is stale but can be served while revalidating
                cache_status = "HIT"
                if self._is_cache_stale(cached_result):
                    cache_status = "STALE"
                    # Trigger async refresh
                    asyncio.create_task(self._refresh_cache(symbol))
                
                cached_result['cache_status'] = cache_status
                return UnifiedQuote(**cached_result)
        
        self.stats['cache_misses'] += 1
        
        try:
            # Fetch from providers
            provider_data = await self._fetch_from_providers(symbol)
            
            if not provider_data:
                logger.warning(f"No data available for {symbol}")
                return self._create_empty_quote(symbol)
            
            # Merge data
            merged_data = self.data_merger.merge_provider_data(symbol, provider_data)
            
            # Calculate data quality
            quality_info = self._assess_data_quality(merged_data)
            merged_data.update(quality_info)
            
            # Perform analysis
            if self.analysis_config.enable_advanced_analysis:
                analysis_results = self.analysis_engine.analyze_quote(merged_data)
                merged_data.update(analysis_results)
            
            # Build unified quote
            quote = UnifiedQuote(**merged_data)
            
            # Add processing metrics
            processing_time = (time.time() - start_time) * 1000
            quote.processing_time_ms = processing_time
            quote.cache_status = "MISS"
            
            # Update stats
            self._update_stats(processing_time)
            
            # Cache the result
            if use_cache:
                await self.cache_manager.set(
                    cache_key,
                    quote.model_dump(),
                    self.cache_config.ttl
                )
            
            logger.info(f"Processed {symbol} in {processing_time:.2f}ms")
            return quote
            
        except Exception as e:
            self.stats['errors'] += 1
            logger.error(f"Error processing {symbol}: {e}")
            raise
    
    async def _fetch_from_providers(
        self, 
        symbol: str
    ) -> List[Tuple[str, Dict[str, Any], QuoteSourceInfo]]:
        """Fetch data from all providers concurrently"""
        tasks = []
        
        for provider in self.providers:
            if not provider.config.enabled:
                continue
            
            task = self._fetch_single_provider(provider, symbol)
            tasks.append(task)
        
        if not tasks:
            return []
        
        # Execute all provider calls concurrently
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Process results
        provider_data = []
        for result in results:
            if isinstance(result, Exception):
                logger.warning(f"Provider error: {result}")
                continue
            
            if result:
                provider_name, data, source_info = result
                if data:
                    provider_data.append((provider_name, data, source_info))
        
        return provider_data
    
    async def _fetch_single_provider(
        self,
        provider: DataProvider,
        symbol: str
    ) -> Optional[Tuple[str, Dict[str, Any], QuoteSourceInfo]]:
        """Fetch data from a single provider"""
        try:
            async with provider:
                data = await provider.fetch_with_retry(symbol)
                
                if not data:
                    return None
                
                # Create source info
                source_info = QuoteSourceInfo(
                    provider=provider.config.name,
                    timestamp=datetime.now(timezone.utc),
                    fields=list(data.keys()),
                    confidence=provider.config.weight,
                    latency_ms=provider.get_stats().get('avg_latency')
                )
                
                return provider.config.name, data, source_info
                
        except Exception as e:
            logger.error(f"Error fetching from {provider.config.name}: {e}")
            return None
    
    def _assess_data_quality(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Assess data quality and identify gaps"""
        # Define field categories and weights
        field_categories = {
            'essential': ['price', 'prev_close', 'volume', 'market_cap'],
            'important': ['open', 'high', 'low', 'pe_ttm', 'eps_ttm'],
            'additional': ['roe', 'profit_margin', 'debt_to_equity', 'revenue_growth_yoy']
        }
        
        weights = {
            'essential': 0.5,
            'important': 0.3,
            'additional': 0.2
        }
        
        # Calculate coverage
        total_score = 0.0
        max_score = 0.0
        gaps = []
        
        for category, fields in field_categories.items():
            category_weight = weights[category]
            fields_present = 0
            
            for field in fields:
                if data.get(field) is not None:
                    fields_present += 1
                else:
                    gaps.append(field)
            
            category_score = (fields_present / len(fields)) * category_weight
            total_score += category_score
            max_score += category_weight
        
        # Calculate quality score (0-1)
        if max_score > 0:
            quality_score = total_score / max_score
        else:
            quality_score = 0.0
        
        # Determine quality level
        if quality_score >= 0.9:
            quality_level = DataQualityLevel.EXCELLENT
        elif quality_score >= 0.75:
            quality_level = DataQualityLevel.GOOD
        elif quality_score >= 0.5:
            quality_level = DataQualityLevel.FAIR
        elif quality_score >= 0.25:
            quality_level = DataQualityLevel.POOR
        else:
            quality_level = DataQualityLevel.MISSING
        
        return {
            'data_quality': quality_level,
            'data_quality_score': quality_score,
            'data_gaps': gaps
        }
    
    def _create_empty_quote(self, symbol: str) -> UnifiedQuote:
        """Create empty quote for missing data"""
        return UnifiedQuote(
            symbol=symbol,
            data_quality=DataQualityLevel.MISSING,
            data_quality_score=0.0,
            data_gaps=["No data from any provider"],
            last_updated_utc=datetime.now(timezone.utc)
        )
    
    def _is_cache_stale(self, cached_data: Dict) -> bool:
        """Check if cached data is stale"""
        if 'last_updated_utc' not in cached_data:
            return True
        
        try:
            last_updated = datetime.fromisoformat(cached_data['last_updated_utc'])
            age = (datetime.now(timezone.utc) - last_updated).seconds
            return age > self.cache_config.ttl
        except:
            return True
    
    async def _refresh_cache(self, symbol: str):
        """Refresh cache in background"""
        try:
            await self.get_enriched_quote(symbol, use_cache=False)
            logger.info(f"Background cache refresh for {symbol}")
        except Exception as e:
            logger.warning(f"Failed to refresh cache for {symbol}: {e}")
    
    def _update_stats(self, processing_time: float):
        """Update engine statistics"""
        total_requests = self.stats['total_requests']
        current_avg = self.stats['avg_processing_time']
        
        self.stats['avg_processing_time'] = (
            (current_avg * (total_requests - 1) + processing_time) 
            / total_requests
        )
    
    def get_stats(self) -> Dict[str, Any]:
        """Get engine statistics"""
        stats = self.stats.copy()
        
        # Add provider stats
        stats['providers'] = {}
        for provider in self.providers:
            stats['providers'][provider.config.name] = provider.get_stats()
        
        # Add cache stats
        stats['cache_hit_rate'] = (
            (self.stats['cache_hits'] / self.stats['total_requests'] * 100)
            if self.stats['total_requests'] > 0 else 0
        )
        
        return stats
    
    async def close(self):
        """Cleanup resources"""
        for provider in self.providers:
            if hasattr(provider, 'session') and provider.session:
                await provider.session.close()
        
        if self.cache_manager.redis:
            await self.cache_manager.redis.close()
        
        logger.info("Data Engine closed")

# ---------------------------------------------------------------------------
# 9) SIMPLIFIED ENTRY POINT (for backward compatibility)
# ---------------------------------------------------------------------------

async def get_enriched_quote(
    symbol: str,
    cache_ttl: int = DEFAULT_CACHE_TTL,
    enable_advanced_analysis: bool = True
) -> UnifiedQuote:
    """
    Simplified entry point for backward compatibility
    
    Args:
        symbol: Stock symbol
        cache_ttl: Cache TTL in seconds
        enable_advanced_analysis: Enable advanced analysis
    
    Returns:
        UnifiedQuote object
    """
    engine = DataEngine(
        cache_config=CacheConfig(ttl=cache_ttl),
        analysis_config=AnalysisConfig(
            enable_advanced_analysis=enable_advanced_analysis
        )
    )
    
    await engine.initialize()
    
    try:
        result = await engine.get_enriched_quote(symbol)
        return result
    finally:
        await engine.close()

# ---------------------------------------------------------------------------
# 10) EXAMPLE USAGE
# ---------------------------------------------------------------------------

async def example_usage():
    """Example usage of the enhanced data engine"""
    
    # Method 1: Simple usage (backward compatible)
    print("=== Method 1: Simple Usage ===")
    result = await get_enriched_quote("MSFT")
    print(f"Symbol: {result.symbol}")
    print(f"Price: {result.price}")
    print(f"Opportunity Score: {result.opportunity_score}")
    print(f"Primary Signal: {result.primary_signal}")
    print(f"Summary: {result.summary}")
    
    # Method 2: Full engine usage
    print("\n=== Method 2: Full Engine Usage ===")
    
    # Create engine with custom configuration
    engine = DataEngine(
        cache_config=CacheConfig(
            enabled=True,
            ttl=300,
            redis_url="redis://localhost:6379"  # Optional
        ),
        analysis_config=AnalysisConfig(
            enable_advanced_analysis=True,
            enable_sector_comparison=True
        ),
        enable_providers=["eodhd", "tadawul"]  # Only use these providers
    )
    
    await engine.initialize()
    
    try:
        # Get quote for US stock
        us_stock = await engine.get_enriched_quote("AAPL")
        print(f"AAPL Analysis:")
        print(f"  Value Score: {us_stock.value_score}")
        print(f"  Growth Score: {us_stock.growth_score}")
        print(f"  Risk Flags: {', '.join(us_stock.risk_flags)}")
        
        # Get quote for KSA stock
        ksa_stock = await engine.get_enriched_quote("1180.SR")
        print(f"\n1180.SR Analysis:")
        print(f"  Market Region: {ksa_stock.market_region}")
        print(f"  Opportunity Score: {ksa_stock.opportunity_score}")
        
        # Get engine statistics
        stats = engine.get_stats()
        print(f"\nEngine Statistics:")
        print(f"  Total Requests: {stats['total_requests']}")
        print(f"  Cache Hit Rate: {stats['cache_hit_rate']:.1f}%")
        
    finally:
        await engine.close()

if __name__ == "__main__":
    # Run example
    asyncio.run(example_usage())
